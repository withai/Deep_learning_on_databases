{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "4embtkV0pNxM"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 4\n",
    "------------\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb` and `3_regularization.ipynb`, we trained fully connected networks to classify [notMNIST](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html) characters.\n",
    "\n",
    "The goal of this assignment is make the neural network convolutional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "id": "tm2CQN_Cpwj0"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 11948,
     "status": "ok",
     "timestamp": 1446658914837,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "016b1a51-0290-4b08-efdb-8c95ffc3cd01"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a TensorFlow-friendly shape:\n",
    "- convolutions need the image data formatted as a cube (width by height by #channels)\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 11952,
     "status": "ok",
     "timestamp": 1446658914857,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "650a208c-8359-4852-f4f5-8bf10e80ef6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28, 1) (200000, 10)\n",
      "Validation set (10000, 28, 28, 1) (10000, 10)\n",
      "Test set (10000, 28, 28, 1) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "num_channels = 1 # grayscale\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape(\n",
    "    (-1, image_size, image_size, num_channels)).astype(np.float32)\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "AgQDIREv02p1"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "5rhgjmROXu2O"
   },
   "source": [
    "Let's build a small network with two convolutional layers, followed by one fully connected layer. Convolutional networks are more expensive computationally, so we'll limit its depth and number of fully connected nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "id": "IZYv70SvvOan"
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "patch_size = 5\n",
    "depth = 16\n",
    "num_hidden = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  tf_train_dataset = tf.placeholder(\n",
    "    tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  layer1_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size, patch_size, num_channels, depth], stddev=0.1))\n",
    "  layer1_biases = tf.Variable(tf.zeros([depth]))\n",
    "  layer2_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size, patch_size, depth, depth], stddev=0.1))\n",
    "  layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))\n",
    "  layer3_weights = tf.Variable(tf.truncated_normal(\n",
    "      [image_size // 4 * image_size // 4 * depth, num_hidden], stddev=0.1))\n",
    "  layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
    "  layer4_weights = tf.Variable(tf.truncated_normal(\n",
    "      [num_hidden, num_labels], stddev=0.1))\n",
    "  layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  \n",
    "  # Model.\n",
    "  def model(data):\n",
    "    conv = tf.nn.conv2d(data, layer1_weights, [1, 2, 2, 1], padding='SAME')\n",
    "    hidden = tf.nn.relu(conv + layer1_biases)\n",
    "    conv = tf.nn.conv2d(hidden, layer2_weights, [1, 2, 2, 1], padding='SAME')\n",
    "    hidden = tf.nn.relu(conv + layer2_biases)\n",
    "    \n",
    "    shape = hidden.get_shape().as_list()\n",
    "    reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "    hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n",
    "    return tf.matmul(hidden, layer4_weights) + layer4_biases\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = model(tf_train_dataset)\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "    \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    "  test_prediction = tf.nn.softmax(model(tf_test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 37
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 63292,
     "status": "ok",
     "timestamp": 1446658966251,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "noKFb2UovVFR",
    "outputId": "28941338-2ef9-4088-8bd1-44295661e628"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 3.144497\n",
      "Minibatch accuracy: 18.8%\n",
      "Validation accuracy: 10.0%\n",
      "Minibatch loss at step 50: 1.042096\n",
      "Minibatch accuracy: 56.2%\n",
      "Validation accuracy: 39.5%\n",
      "Minibatch loss at step 100: 0.687427\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 51.4%\n",
      "Minibatch loss at step 150: 1.168226\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 56.7%\n",
      "Minibatch loss at step 200: 0.875445\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 59.3%\n",
      "Minibatch loss at step 250: 1.082195\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 57.9%\n",
      "Minibatch loss at step 300: 1.161536\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 58.8%\n",
      "Minibatch loss at step 350: 0.509612\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 62.0%\n",
      "Minibatch loss at step 400: 1.032709\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 60.9%\n",
      "Minibatch loss at step 450: 0.550442\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 64.8%\n",
      "Minibatch loss at step 500: 0.658947\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 62.2%\n",
      "Minibatch loss at step 550: 0.333687\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 63.8%\n",
      "Minibatch loss at step 600: 0.459412\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 64.4%\n",
      "Minibatch loss at step 650: 0.453397\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 65.8%\n",
      "Minibatch loss at step 700: 0.175111\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 64.7%\n",
      "Minibatch loss at step 750: 0.373306\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 65.4%\n",
      "Minibatch loss at step 800: 0.698938\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 65.7%\n",
      "Minibatch loss at step 850: 0.239579\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 66.0%\n",
      "Minibatch loss at step 900: 0.770743\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 64.3%\n",
      "Minibatch loss at step 950: 0.012678\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 67.0%\n",
      "Minibatch loss at step 1000: 0.403866\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 65.9%\n",
      "Test accuracy: 88.0%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 1001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  for step in range(num_steps):\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 50 == 0):\n",
    "      print('Minibatch loss at step %d: %f' % (step, l))\n",
    "      print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "      print('Validation accuracy: %.1f%%' % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "KedKkn4EutIK"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "The convolutional model above uses convolutions with stride 2 to reduce the dimensionality. Replace the strides by a max pooling operation (`nn.max_pool()`) of stride 2 and kernel size 2.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "patch_size = 5\n",
    "depth = 16\n",
    "num_hidden = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  tf_train_dataset = tf.placeholder(\n",
    "    tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  layer1_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size, patch_size, num_channels, depth], stddev=0.1))\n",
    "  layer1_biases = tf.Variable(tf.zeros([depth]))\n",
    "  layer2_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size, patch_size, depth, depth], stddev=0.1))\n",
    "  layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))\n",
    "  layer3_weights = tf.Variable(tf.truncated_normal(\n",
    "      [image_size // 4 * image_size // 4 * depth, num_hidden], stddev=0.1))\n",
    "  layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
    "  layer4_weights = tf.Variable(tf.truncated_normal(\n",
    "      [num_hidden, num_labels], stddev=0.1))\n",
    "  layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  \n",
    "  # Model.\n",
    "  def model(data):\n",
    "    conv1 = tf.nn.conv2d(data, layer1_weights, [1, 1, 1, 1], padding='SAME')\n",
    "    hidden1 = tf.nn.relu(conv1 + layer1_biases)\n",
    "    pool1 = tf.nn.max_pool(hidden1, [1, 2, 2, 1], [1, 2, 2, 1], padding='SAME')\n",
    "    conv2 = tf.nn.conv2d(pool1, layer2_weights, [1, 1, 1, 1], padding='SAME')\n",
    "    hidden2 = tf.nn.relu(conv2 + layer2_biases)\n",
    "    pool2 = tf.nn.max_pool(hidden2, [1, 2, 2, 1], [1, 2, 2, 1], padding='SAME')\n",
    "    \n",
    "    shape = pool2.get_shape().as_list()\n",
    "    reshape = tf.reshape(pool2, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "    hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n",
    "    return tf.matmul(hidden, layer4_weights) + layer4_biases\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = model(tf_train_dataset)\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "    \n",
    "  # Optimizer.\n",
    "  #optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  optimizer = tf.train.AdamOptimizer().minimize(loss)\n",
    "\n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    "  test_prediction = tf.nn.softmax(model(tf_test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 3.732943\n",
      "Minibatch accuracy: 6.2%\n",
      "Validation accuracy: 9.8%\n",
      "Minibatch loss at step 50: 1.914641\n",
      "Minibatch accuracy: 25.0%\n",
      "Validation accuracy: 35.7%\n",
      "Minibatch loss at step 100: 0.756335\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 53.0%\n",
      "Minibatch loss at step 150: 1.229692\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 53.9%\n",
      "Minibatch loss at step 200: 1.037723\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 59.2%\n",
      "Minibatch loss at step 250: 1.329782\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 62.6%\n",
      "Minibatch loss at step 300: 1.134509\n",
      "Minibatch accuracy: 56.2%\n",
      "Validation accuracy: 61.3%\n",
      "Minibatch loss at step 350: 0.770173\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 62.9%\n",
      "Minibatch loss at step 400: 1.122269\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 62.4%\n",
      "Minibatch loss at step 450: 0.601667\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 66.4%\n",
      "Minibatch loss at step 500: 0.856818\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 64.7%\n",
      "Minibatch loss at step 550: 0.415978\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 65.9%\n",
      "Minibatch loss at step 600: 0.268905\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 65.7%\n",
      "Minibatch loss at step 650: 0.316597\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 67.8%\n",
      "Minibatch loss at step 700: 0.242276\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 67.3%\n",
      "Minibatch loss at step 750: 0.412453\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 67.5%\n",
      "Minibatch loss at step 800: 0.668851\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 69.9%\n",
      "Minibatch loss at step 850: 0.271914\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 69.0%\n",
      "Minibatch loss at step 900: 0.759136\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 68.1%\n",
      "Minibatch loss at step 950: 0.028207\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.0%\n",
      "Minibatch loss at step 1000: 0.246298\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 66.7%\n",
      "Test accuracy: 89.3%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 1001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  for step in range(num_steps):\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 50 == 0):\n",
    "      print('Minibatch loss at step %d: %f' % (step, l))\n",
    "      print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "      print('Validation accuracy: %.1f%%' % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "klf21gpbAgb-"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a convolutional net. Look for example at the classic [LeNet5](http://yann.lecun.com/exdb/lenet/) architecture, adding Dropout, and/or adding learning rate decay.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Model designed using dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Conv2D:0\", shape=(16, 28, 28, 16), dtype=float32)\n",
      "Tensor(\"Variable/read:0\", shape=(5, 5, 1, 16), dtype=float32)\n",
      "Tensor(\"Conv2D_1:0\", shape=(16, 14, 14, 16), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "patch_size = 5\n",
    "depth = 16\n",
    "num_hidden = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  tf_train_dataset = tf.placeholder(\n",
    "    tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  layer1_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size, patch_size, num_channels, depth], stddev=0.1))\n",
    "  layer1_biases = tf.Variable(tf.zeros([depth]))\n",
    "  layer2_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size, patch_size, depth, depth], stddev=0.1))\n",
    "  layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))\n",
    "  layer3_weights = tf.Variable(tf.truncated_normal(\n",
    "      [image_size // 4 * image_size // 4 * depth, num_hidden], stddev=0.1))\n",
    "  layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
    "  layer4_weights = tf.Variable(tf.truncated_normal(\n",
    "      [num_hidden, num_labels], stddev=0.1))\n",
    "  layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  \n",
    "  # Model.\n",
    "  def modelTrain(data):\n",
    "    conv1 = tf.nn.conv2d(data, layer1_weights, [1, 1, 1, 1], padding='SAME')\n",
    "    print(conv1)\n",
    "    print(layer1_weights)\n",
    "    hidden1 = tf.nn.relu(conv1 + layer1_biases)\n",
    "    pool1 = tf.nn.max_pool(hidden1, [1, 2, 2, 1], [1, 2, 2, 1], padding='SAME')\n",
    "    drop1 = tf.nn.dropout(pool1, 0.5)\n",
    "    \n",
    "    conv2 = tf.nn.conv2d(drop1, layer2_weights, [1, 1, 1, 1], padding='SAME')\n",
    "    print(conv2)\n",
    "    hidden2 = tf.nn.relu(conv2 + layer2_biases)\n",
    "    pool2 = tf.nn.max_pool(hidden2, [1, 2, 2, 1], [1, 2, 2, 1], padding='SAME')\n",
    "    drop2 = tf.nn.dropout(pool2, 0.5)\n",
    "    \n",
    "    shape = drop2.get_shape().as_list()\n",
    "    reshape = tf.reshape(drop2, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "    hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n",
    "    return tf.matmul(hidden, layer4_weights) + layer4_biases\n",
    "\n",
    "  def modelTest(data):\n",
    "    conv1 = tf.nn.conv2d(data, layer1_weights, [1, 1, 1, 1], padding='SAME')\n",
    "    hidden1 = tf.nn.relu(conv1 + layer1_biases)\n",
    "    pool1 = tf.nn.max_pool(hidden1, [1, 2, 2, 1], [1, 2, 2, 1], padding='SAME')\n",
    "    \n",
    "    conv2 = tf.nn.conv2d(pool1, layer2_weights, [1, 1, 1, 1], padding='SAME')\n",
    "    hidden2 = tf.nn.relu(conv2 + layer2_biases)\n",
    "    pool2 = tf.nn.max_pool(hidden2, [1, 2, 2, 1], [1, 2, 2, 1], padding='SAME')\n",
    "    \n",
    "    shape = pool2.get_shape().as_list()\n",
    "    reshape = tf.reshape(pool2, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "    hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n",
    "    return tf.matmul(hidden, layer4_weights) + layer4_biases\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = modelTrain(tf_train_dataset)\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "    \n",
    "  # Optimizer.\n",
    "  #optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  optimizer = tf.train.AdamOptimizer().minimize(loss)\n",
    "\n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(modelTest(tf_valid_dataset))\n",
    "  test_prediction = tf.nn.softmax(modelTest(tf_test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 4.866991\n",
      "Minibatch accuracy: 12.5%\n",
      "Validation accuracy: 10.3%\n",
      "Minibatch loss at step 50: 2.521278\n",
      "Minibatch accuracy: 12.5%\n",
      "Validation accuracy: 11.3%\n",
      "Minibatch loss at step 100: 2.082868\n",
      "Minibatch accuracy: 18.8%\n",
      "Validation accuracy: 22.5%\n",
      "Minibatch loss at step 150: 1.682335\n",
      "Minibatch accuracy: 43.8%\n",
      "Validation accuracy: 31.3%\n",
      "Minibatch loss at step 200: 1.596248\n",
      "Minibatch accuracy: 50.0%\n",
      "Validation accuracy: 40.1%\n",
      "Minibatch loss at step 250: 1.884851\n",
      "Minibatch accuracy: 56.2%\n",
      "Validation accuracy: 51.6%\n",
      "Minibatch loss at step 300: 1.552370\n",
      "Minibatch accuracy: 43.8%\n",
      "Validation accuracy: 53.9%\n",
      "Minibatch loss at step 350: 0.788536\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 55.3%\n",
      "Minibatch loss at step 400: 1.327368\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 58.1%\n",
      "Minibatch loss at step 450: 1.426111\n",
      "Minibatch accuracy: 43.8%\n",
      "Validation accuracy: 59.8%\n",
      "Minibatch loss at step 500: 1.693400\n",
      "Minibatch accuracy: 50.0%\n",
      "Validation accuracy: 57.5%\n",
      "Minibatch loss at step 550: 0.517029\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 61.5%\n",
      "Minibatch loss at step 600: 0.784655\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 62.2%\n",
      "Minibatch loss at step 650: 0.558506\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 62.2%\n",
      "Minibatch loss at step 700: 0.452831\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 62.1%\n",
      "Minibatch loss at step 750: 0.741022\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 61.4%\n",
      "Minibatch loss at step 800: 1.134803\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 64.1%\n",
      "Minibatch loss at step 850: 0.558163\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 65.1%\n",
      "Minibatch loss at step 900: 0.838288\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 65.1%\n",
      "Minibatch loss at step 950: 0.204518\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 64.5%\n",
      "Minibatch loss at step 1000: 0.543961\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 64.8%\n",
      "Test accuracy: 88.5%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 1001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  for step in range(num_steps):\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 50 == 0):\n",
    "      print('Minibatch loss at step %d: %f' % (step, l))\n",
    "      print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "      print('Validation accuracy: %.1f%%' % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "Model designed using 5X5 convolution, pooling followed by 5x5 convolution, pooling and finally a fully connected layer with 64 neurons "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "patch_size = 5\n",
    "depth = 14\n",
    "num_hidden = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  tf_train_dataset = tf.placeholder(\n",
    "    tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  layer1_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size, patch_size, num_channels, depth], stddev=0.1))    # [5, 5, 1, 14]\n",
    "  layer1_biases = tf.Variable(tf.truncated_normal([depth]))\n",
    "  layer2_weights = tf.Variable(tf.truncated_normal(     \n",
    "      [patch_size, patch_size, depth, depth], stddev=0.1))           # [5, 5, 14, 28]  \n",
    "  layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))\n",
    "  layer3_weights = tf.Variable(tf.truncated_normal(\n",
    "      [image_size//4 * image_size//4 * depth, num_hidden], stddev=0.1))\n",
    "  layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
    "  layer4_weights = tf.Variable(tf.truncated_normal(\n",
    "      [num_hidden, num_labels], stddev=0.1))\n",
    "  layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  \n",
    "  # Model.\n",
    "  def model(data):\n",
    "    conv1 = tf.nn.conv2d(data, layer1_weights, [1, 1, 1, 1], padding='SAME')\n",
    "    hidden1 = tf.nn.relu(conv1 + layer1_biases)\n",
    "    pool1 = tf.nn.avg_pool(hidden1, [1, 2, 2, 1], [1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "    conv2 = tf.nn.conv2d(pool1, layer2_weights, [1, 1, 1, 1], padding='SAME')\n",
    "    hidden2 = tf.nn.relu(conv2 + layer2_biases)\n",
    "    pool2 = tf.nn.avg_pool(hidden2, [1, 2, 2, 1], [1, 2, 2, 1], padding='SAME')\n",
    "    \n",
    "    shape = pool2.get_shape().as_list()\n",
    "    reshape = tf.reshape(pool2, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "    \n",
    "    hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n",
    "    return tf.matmul(hidden, layer4_weights) + layer4_biases\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = model(tf_train_dataset)\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "    \n",
    "  # Optimizer.\n",
    "  #optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  optimizer = tf.train.AdamOptimizer().minimize(loss)\n",
    "\n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    "  test_prediction = tf.nn.softmax(model(tf_test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 4.427645\n",
      "Minibatch accuracy: 6.2%\n",
      "Validation accuracy: 10.0%\n",
      "Minibatch loss at step 50: 2.245952\n",
      "Minibatch accuracy: 12.5%\n",
      "Validation accuracy: 13.0%\n",
      "Minibatch loss at step 100: 1.695200\n",
      "Minibatch accuracy: 37.5%\n",
      "Validation accuracy: 38.3%\n",
      "Minibatch loss at step 150: 1.252712\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 43.5%\n",
      "Minibatch loss at step 200: 1.168112\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 55.0%\n",
      "Minibatch loss at step 250: 1.267542\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 62.2%\n",
      "Minibatch loss at step 300: 1.290528\n",
      "Minibatch accuracy: 56.2%\n",
      "Validation accuracy: 62.7%\n",
      "Minibatch loss at step 350: 0.611996\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 64.8%\n",
      "Minibatch loss at step 400: 1.081666\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 61.5%\n",
      "Minibatch loss at step 450: 0.732446\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 65.1%\n",
      "Minibatch loss at step 500: 0.968014\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 63.6%\n",
      "Minibatch loss at step 550: 0.582338\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 66.3%\n",
      "Minibatch loss at step 600: 0.356759\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 64.8%\n",
      "Minibatch loss at step 650: 0.450525\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 67.2%\n",
      "Minibatch loss at step 700: 0.173856\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 67.0%\n",
      "Minibatch loss at step 750: 0.443787\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 67.9%\n",
      "Minibatch loss at step 800: 0.637509\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 66.2%\n",
      "Minibatch loss at step 850: 0.403760\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 67.8%\n",
      "Minibatch loss at step 900: 0.593062\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 68.6%\n",
      "Minibatch loss at step 950: 0.011538\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.6%\n",
      "Minibatch loss at step 1000: 0.319254\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 67.0%\n",
      "Minibatch loss at step 1050: 0.384354\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 69.0%\n",
      "Minibatch loss at step 1100: 0.637833\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 69.4%\n",
      "Minibatch loss at step 1150: 0.875837\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 70.5%\n",
      "Minibatch loss at step 1200: 0.364385\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 71.3%\n",
      "Minibatch loss at step 1250: 0.230906\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 71.3%\n",
      "Minibatch loss at step 1300: 0.133826\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 69.8%\n",
      "Minibatch loss at step 1350: 0.401399\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 70.2%\n",
      "Minibatch loss at step 1400: 0.930814\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 71.6%\n",
      "Minibatch loss at step 1450: 0.427417\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 71.9%\n",
      "Minibatch loss at step 1500: 0.436300\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 70.2%\n",
      "Minibatch loss at step 1550: 0.596973\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 72.3%\n",
      "Minibatch loss at step 1600: 0.609855\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 73.2%\n",
      "Minibatch loss at step 1650: 0.636115\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 72.0%\n",
      "Minibatch loss at step 1700: 0.261390\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 70.7%\n",
      "Minibatch loss at step 1750: 0.329484\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 73.3%\n",
      "Minibatch loss at step 1800: 0.537336\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 73.2%\n",
      "Minibatch loss at step 1850: 0.431089\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 72.6%\n",
      "Minibatch loss at step 1900: 0.029857\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 71.8%\n",
      "Minibatch loss at step 1950: 0.315402\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 73.0%\n",
      "Minibatch loss at step 2000: 0.555667\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 73.4%\n",
      "Minibatch loss at step 2050: 0.415133\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 74.0%\n",
      "Minibatch loss at step 2100: 0.991429\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 73.1%\n",
      "Minibatch loss at step 2150: 0.342955\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 73.6%\n",
      "Minibatch loss at step 2200: 0.691884\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 74.2%\n",
      "Minibatch loss at step 2250: 0.173290\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 73.0%\n",
      "Minibatch loss at step 2300: 1.170257\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 75.2%\n",
      "Minibatch loss at step 2350: 0.481725\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 73.4%\n",
      "Minibatch loss at step 2400: 0.393746\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 73.8%\n",
      "Minibatch loss at step 2450: 0.583781\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 74.5%\n",
      "Minibatch loss at step 2500: 0.087000\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 74.4%\n",
      "Minibatch loss at step 2550: 0.377418\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 73.5%\n",
      "Minibatch loss at step 2600: 0.272332\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 73.3%\n",
      "Minibatch loss at step 2650: 0.469042\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 72.8%\n",
      "Minibatch loss at step 2700: 0.678971\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 74.6%\n",
      "Minibatch loss at step 2750: 0.238929\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 73.5%\n",
      "Minibatch loss at step 2800: 0.692318\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 74.5%\n",
      "Minibatch loss at step 2850: 0.577681\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 74.3%\n",
      "Minibatch loss at step 2900: 0.218299\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 74.2%\n",
      "Minibatch loss at step 2950: 0.971214\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 74.1%\n",
      "Minibatch loss at step 3000: 0.305811\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 72.9%\n",
      "Minibatch loss at step 3050: 0.375839\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 74.5%\n",
      "Minibatch loss at step 3100: 0.216986\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 75.3%\n",
      "Minibatch loss at step 3150: 0.277522\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 73.9%\n",
      "Minibatch loss at step 3200: 0.081377\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 73.3%\n",
      "Minibatch loss at step 3250: 0.707432\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 74.9%\n",
      "Minibatch loss at step 3300: 0.780199\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 75.9%\n",
      "Minibatch loss at step 3350: 0.586100\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 74.9%\n",
      "Minibatch loss at step 3400: 0.235172\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 75.6%\n",
      "Minibatch loss at step 3450: 0.434204\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 73.0%\n",
      "Minibatch loss at step 3500: 0.477856\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 72.9%\n",
      "Minibatch loss at step 3550: 0.720913\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 74.8%\n",
      "Minibatch loss at step 3600: 0.762456\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 75.3%\n",
      "Minibatch loss at step 3650: 0.377284\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 75.7%\n",
      "Minibatch loss at step 3700: 0.459663\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 75.4%\n",
      "Minibatch loss at step 3750: 0.492148\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 74.8%\n",
      "Minibatch loss at step 3800: 0.026819\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 74.0%\n",
      "Minibatch loss at step 3850: 0.521676\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 76.2%\n",
      "Minibatch loss at step 3900: 0.428363\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 74.5%\n",
      "Minibatch loss at step 3950: 0.127346\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 75.7%\n",
      "Minibatch loss at step 4000: 0.722437\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 75.2%\n",
      "Minibatch loss at step 4050: 0.370515\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 75.4%\n",
      "Minibatch loss at step 4100: 0.584966\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 76.1%\n",
      "Minibatch loss at step 4150: 0.302172\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 74.8%\n",
      "Minibatch loss at step 4200: 0.246771\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 76.7%\n",
      "Minibatch loss at step 4250: 0.574557\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 74.6%\n",
      "Minibatch loss at step 4300: 0.083309\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 76.2%\n",
      "Minibatch loss at step 4350: 0.717233\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 75.8%\n",
      "Minibatch loss at step 4400: 0.418663\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 76.0%\n",
      "Minibatch loss at step 4450: 0.716173\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 75.8%\n",
      "Minibatch loss at step 4500: 0.266092\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 75.0%\n",
      "Minibatch loss at step 4550: 0.364338\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 76.0%\n",
      "Minibatch loss at step 4600: 0.390808\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 74.5%\n",
      "Minibatch loss at step 4650: 0.122932\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 76.0%\n",
      "Minibatch loss at step 4700: 0.407304\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 75.4%\n",
      "Minibatch loss at step 4750: 0.203189\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 76.2%\n",
      "Minibatch loss at step 4800: 0.335578\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 76.0%\n",
      "Minibatch loss at step 4850: 0.216620\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 76.5%\n",
      "Minibatch loss at step 4900: 0.458809\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 75.7%\n",
      "Minibatch loss at step 4950: 0.324617\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 73.2%\n",
      "Minibatch loss at step 5000: 0.137560\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 76.2%\n",
      "Minibatch loss at step 5050: 0.273410\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 75.8%\n",
      "Minibatch loss at step 5100: 0.403785\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 75.7%\n",
      "Minibatch loss at step 5150: 0.688758\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 75.7%\n",
      "Minibatch loss at step 5200: 0.077685\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 76.4%\n",
      "Minibatch loss at step 5250: 0.389625\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 76.8%\n",
      "Minibatch loss at step 5300: 0.301336\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 77.6%\n",
      "Minibatch loss at step 5350: 0.669956\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 77.0%\n",
      "Minibatch loss at step 5400: 0.028306\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 76.6%\n",
      "Minibatch loss at step 5450: 0.689733\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 76.9%\n",
      "Minibatch loss at step 5500: 0.153987\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 76.2%\n",
      "Minibatch loss at step 5550: 0.271435\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 76.6%\n",
      "Minibatch loss at step 5600: 0.116002\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 76.8%\n",
      "Minibatch loss at step 5650: 0.230094\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 77.1%\n",
      "Minibatch loss at step 5700: 0.273991\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 77.1%\n",
      "Minibatch loss at step 5750: 0.485531\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 77.0%\n",
      "Minibatch loss at step 5800: 0.247694\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 75.3%\n",
      "Minibatch loss at step 5850: 0.618783\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 76.8%\n",
      "Minibatch loss at step 5900: 0.349513\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 76.6%\n",
      "Minibatch loss at step 5950: 0.681085\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 75.4%\n",
      "Minibatch loss at step 6000: 0.328616\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 75.5%\n",
      "Minibatch loss at step 6050: 0.168600\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 76.7%\n",
      "Minibatch loss at step 6100: 0.716487\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 76.5%\n",
      "Minibatch loss at step 6150: 0.617149\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 76.1%\n",
      "Minibatch loss at step 6200: 0.162132\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 75.9%\n",
      "Minibatch loss at step 6250: 0.487228\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 77.1%\n",
      "Minibatch loss at step 6300: 0.154408\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 76.1%\n",
      "Minibatch loss at step 6350: 0.161656\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 76.8%\n",
      "Minibatch loss at step 6400: 0.170435\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 77.0%\n",
      "Minibatch loss at step 6450: 0.318549\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 77.8%\n",
      "Minibatch loss at step 6500: 0.070946\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 75.8%\n",
      "Minibatch loss at step 6550: 0.186816\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 76.1%\n",
      "Minibatch loss at step 6600: 0.669181\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 76.6%\n",
      "Minibatch loss at step 6650: 0.380614\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 77.1%\n",
      "Minibatch loss at step 6700: 0.173680\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 76.0%\n",
      "Minibatch loss at step 6750: 0.041043\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 77.0%\n",
      "Minibatch loss at step 6800: 0.228339\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 77.0%\n",
      "Minibatch loss at step 6850: 0.015500\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 77.2%\n",
      "Minibatch loss at step 6900: 0.447517\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 77.0%\n",
      "Minibatch loss at step 6950: 0.052607\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 77.6%\n",
      "Minibatch loss at step 7000: 0.295842\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 76.5%\n",
      "Minibatch loss at step 7050: 0.198672\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 77.2%\n",
      "Minibatch loss at step 7100: 0.426592\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 77.1%\n",
      "Minibatch loss at step 7150: 0.225367\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 76.1%\n",
      "Minibatch loss at step 7200: 0.292695\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 77.6%\n",
      "Minibatch loss at step 7250: 0.059763\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 76.5%\n",
      "Minibatch loss at step 7300: 0.411666\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 76.2%\n",
      "Minibatch loss at step 7350: 0.372732\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 76.5%\n",
      "Minibatch loss at step 7400: 0.034375\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 77.1%\n",
      "Minibatch loss at step 7450: 0.075088\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 77.8%\n",
      "Minibatch loss at step 7500: 0.634682\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 77.1%\n",
      "Minibatch loss at step 7550: 0.548096\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 77.7%\n",
      "Minibatch loss at step 7600: 0.601801\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 78.0%\n",
      "Minibatch loss at step 7650: 0.501780\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 77.4%\n",
      "Minibatch loss at step 7700: 0.441268\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 76.6%\n",
      "Minibatch loss at step 7750: 0.621219\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 76.8%\n",
      "Minibatch loss at step 7800: 0.908257\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 77.4%\n",
      "Minibatch loss at step 7850: 0.336043\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 77.0%\n",
      "Minibatch loss at step 7900: 0.415387\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 77.0%\n",
      "Minibatch loss at step 7950: 0.302553\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 77.6%\n",
      "Minibatch loss at step 8000: 0.171895\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 77.2%\n",
      "Minibatch loss at step 8050: 0.532722\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 77.4%\n",
      "Minibatch loss at step 8100: 0.107544\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 76.7%\n",
      "Minibatch loss at step 8150: 0.379222\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 76.8%\n",
      "Minibatch loss at step 8200: 0.183137\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 77.5%\n",
      "Minibatch loss at step 8250: 0.476858\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 77.0%\n",
      "Minibatch loss at step 8300: 0.147921\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 78.0%\n",
      "Minibatch loss at step 8350: 0.375056\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 77.5%\n",
      "Minibatch loss at step 8400: 0.866349\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 77.7%\n",
      "Minibatch loss at step 8450: 0.100573\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 78.1%\n",
      "Minibatch loss at step 8500: 0.427252\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 77.7%\n",
      "Minibatch loss at step 8550: 0.243011\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 77.7%\n",
      "Minibatch loss at step 8600: 0.332908\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 77.6%\n",
      "Minibatch loss at step 8650: 0.218174\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 77.8%\n",
      "Minibatch loss at step 8700: 0.047391\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 77.5%\n",
      "Minibatch loss at step 8750: 0.346350\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 77.8%\n",
      "Minibatch loss at step 8800: 0.423447\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 78.6%\n",
      "Minibatch loss at step 8850: 0.628812\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 78.0%\n",
      "Minibatch loss at step 8900: 0.001675\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 78.3%\n",
      "Minibatch loss at step 8950: 0.303916\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 78.0%\n",
      "Minibatch loss at step 9000: 0.164655\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 77.5%\n",
      "Minibatch loss at step 9050: 0.088498\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 78.6%\n",
      "Minibatch loss at step 9100: 0.226355\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 78.2%\n",
      "Minibatch loss at step 9150: 0.500464\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 78.0%\n",
      "Minibatch loss at step 9200: 0.455870\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 78.0%\n",
      "Minibatch loss at step 9250: 0.011384\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 77.6%\n",
      "Minibatch loss at step 9300: 0.323196\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 78.3%\n",
      "Minibatch loss at step 9350: 0.113179\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 78.3%\n",
      "Minibatch loss at step 9400: 0.045909\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 78.3%\n",
      "Minibatch loss at step 9450: 0.290255\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 78.4%\n",
      "Minibatch loss at step 9500: 0.071192\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 77.9%\n",
      "Minibatch loss at step 9550: 0.223828\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 77.3%\n",
      "Minibatch loss at step 9600: 0.683210\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 77.3%\n",
      "Minibatch loss at step 9650: 0.202714\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 77.6%\n",
      "Minibatch loss at step 9700: 0.084508\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 78.3%\n",
      "Minibatch loss at step 9750: 0.043990\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 78.3%\n",
      "Minibatch loss at step 9800: 0.335607\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 78.3%\n",
      "Minibatch loss at step 9850: 0.577771\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 78.2%\n",
      "Minibatch loss at step 9900: 0.230205\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 77.8%\n",
      "Minibatch loss at step 9950: 0.691792\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 78.3%\n",
      "Minibatch loss at step 10000: 0.451537\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 78.9%\n",
      "Minibatch loss at step 10050: 0.394097\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 78.5%\n",
      "Minibatch loss at step 10100: 0.263948\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 78.9%\n",
      "Minibatch loss at step 10150: 0.683554\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 78.4%\n",
      "Minibatch loss at step 10200: 0.179602\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 78.3%\n",
      "Minibatch loss at step 10250: 0.236966\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 78.2%\n",
      "Minibatch loss at step 10300: 0.158514\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 78.5%\n",
      "Minibatch loss at step 10350: 0.097647\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 78.5%\n",
      "Minibatch loss at step 10400: 0.396166\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 78.3%\n",
      "Minibatch loss at step 10450: 0.362733\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 78.7%\n",
      "Minibatch loss at step 10500: 0.294616\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 78.8%\n",
      "Minibatch loss at step 10550: 0.132629\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 78.1%\n",
      "Minibatch loss at step 10600: 0.332001\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 78.7%\n",
      "Minibatch loss at step 10650: 0.321840\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 78.4%\n",
      "Minibatch loss at step 10700: 0.353927\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 77.5%\n",
      "Minibatch loss at step 10750: 0.329896\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 78.6%\n",
      "Minibatch loss at step 10800: 0.134884\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 78.8%\n",
      "Minibatch loss at step 10850: 0.558333\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 77.9%\n",
      "Minibatch loss at step 10900: 0.115745\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 78.4%\n",
      "Minibatch loss at step 10950: 0.192249\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 78.2%\n",
      "Minibatch loss at step 11000: 0.414235\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 78.6%\n",
      "Minibatch loss at step 11050: 0.405605\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 78.7%\n",
      "Minibatch loss at step 11100: 0.576685\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 78.8%\n",
      "Minibatch loss at step 11150: 0.199382\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 79.0%\n",
      "Minibatch loss at step 11200: 0.044398\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 78.6%\n",
      "Minibatch loss at step 11250: 0.431660\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 78.8%\n",
      "Minibatch loss at step 11300: 0.205876\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 78.3%\n",
      "Minibatch loss at step 11350: 0.165774\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 78.7%\n",
      "Minibatch loss at step 11400: 0.031653\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 78.5%\n",
      "Minibatch loss at step 11450: 0.350778\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 78.1%\n",
      "Minibatch loss at step 11500: 0.304034\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 78.4%\n",
      "Minibatch loss at step 11550: 0.547085\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 78.5%\n",
      "Minibatch loss at step 11600: 0.225109\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 79.2%\n",
      "Minibatch loss at step 11650: 0.618911\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 78.2%\n",
      "Minibatch loss at step 11700: 0.092815\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 78.5%\n",
      "Minibatch loss at step 11750: 0.258938\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 79.3%\n",
      "Minibatch loss at step 11800: 0.334878\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 78.8%\n",
      "Minibatch loss at step 11850: 0.083406\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 79.2%\n",
      "Minibatch loss at step 11900: 0.665572\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 78.5%\n",
      "Minibatch loss at step 11950: 0.221372\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 78.6%\n",
      "Minibatch loss at step 12000: 0.403789\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 77.9%\n",
      "Minibatch loss at step 12050: 0.287258\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 78.1%\n",
      "Minibatch loss at step 12100: 0.236339\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 78.2%\n",
      "Minibatch loss at step 12150: 0.251338\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 79.0%\n",
      "Minibatch loss at step 12200: 0.319620\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 78.8%\n",
      "Minibatch loss at step 12250: 0.458027\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 78.6%\n",
      "Minibatch loss at step 12300: 0.335115\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 79.3%\n",
      "Minibatch loss at step 12350: 0.476488\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 78.5%\n",
      "Minibatch loss at step 12400: 0.132757\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 79.1%\n",
      "Minibatch loss at step 12450: 1.138309\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 78.5%\n",
      "Minibatch loss at step 12500: 0.260174\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 79.0%\n",
      "Minibatch loss at step 12550: 0.503927\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 78.9%\n",
      "Minibatch loss at step 12600: 0.004061\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 78.9%\n",
      "Minibatch loss at step 12650: 0.557774\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 79.3%\n",
      "Minibatch loss at step 12700: 0.143460\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 78.9%\n",
      "Minibatch loss at step 12750: 0.237537\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 78.5%\n",
      "Minibatch loss at step 12800: 0.508558\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 78.4%\n",
      "Minibatch loss at step 12850: 0.226903\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 78.8%\n",
      "Minibatch loss at step 12900: 0.365727\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 78.4%\n",
      "Minibatch loss at step 12950: 0.311384\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 78.9%\n",
      "Minibatch loss at step 13000: 0.288098\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 79.5%\n",
      "Minibatch loss at step 13050: 0.333149\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 78.9%\n",
      "Minibatch loss at step 13100: 0.257425\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 79.3%\n",
      "Minibatch loss at step 13150: 0.315101\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 78.4%\n",
      "Minibatch loss at step 13200: 0.036305\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 78.8%\n",
      "Minibatch loss at step 13250: 0.003086\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 79.2%\n",
      "Minibatch loss at step 13300: 0.338453\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 78.5%\n",
      "Minibatch loss at step 13350: 0.094696\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 79.3%\n",
      "Minibatch loss at step 13400: 0.338526\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 79.0%\n",
      "Minibatch loss at step 13450: 0.003459\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 78.6%\n",
      "Minibatch loss at step 13500: 0.779151\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 79.0%\n",
      "Minibatch loss at step 13550: 0.563369\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 78.6%\n",
      "Minibatch loss at step 13600: 0.351291\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 79.5%\n",
      "Minibatch loss at step 13650: 0.003252\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 78.9%\n",
      "Minibatch loss at step 13700: 0.081906\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 78.6%\n",
      "Minibatch loss at step 13750: 0.709986\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 78.6%\n",
      "Minibatch loss at step 13800: 0.405460\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 78.0%\n",
      "Minibatch loss at step 13850: 0.443898\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 78.8%\n",
      "Minibatch loss at step 13900: 0.002372\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 79.1%\n",
      "Minibatch loss at step 13950: 0.143017\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 78.8%\n",
      "Minibatch loss at step 14000: 0.405489\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 79.3%\n",
      "Minibatch loss at step 14050: 0.075682\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 78.7%\n",
      "Minibatch loss at step 14100: 0.754258\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 79.5%\n",
      "Minibatch loss at step 14150: 0.318178\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 78.9%\n",
      "Minibatch loss at step 14200: 0.327680\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 79.5%\n",
      "Minibatch loss at step 14250: 0.086594\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 79.0%\n",
      "Minibatch loss at step 14300: 0.199530\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 79.5%\n",
      "Minibatch loss at step 14350: 0.197174\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 79.3%\n",
      "Minibatch loss at step 14400: 0.103180\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 79.3%\n",
      "Minibatch loss at step 14450: 0.001077\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 79.2%\n",
      "Minibatch loss at step 14500: 0.349091\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 79.8%\n",
      "Minibatch loss at step 14550: 0.182508\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 79.2%\n",
      "Minibatch loss at step 14600: 0.514374\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 79.5%\n",
      "Minibatch loss at step 14650: 0.463649\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 79.5%\n",
      "Minibatch loss at step 14700: 0.615806\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 79.3%\n",
      "Minibatch loss at step 14750: 0.220641\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 79.2%\n",
      "Minibatch loss at step 14800: 0.421141\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 79.4%\n",
      "Minibatch loss at step 14850: 0.133823\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 79.8%\n",
      "Minibatch loss at step 14900: 0.084839\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 79.7%\n",
      "Minibatch loss at step 14950: 0.005636\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 79.0%\n",
      "Minibatch loss at step 15000: 0.259427\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 79.8%\n",
      "Test accuracy: 95.5%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 15001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  for step in range(num_steps):\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 50 == 0):\n",
    "      print('Minibatch loss at step %d: %f' % (step, l))\n",
    "      print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "      print('Validation accuracy: %.1f%%' % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Demonstrating model to show dropout does not work on convolutional layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "patch_size = 5\n",
    "depth = 14\n",
    "num_hidden = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  tf_train_dataset = tf.placeholder(\n",
    "    tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  layer1_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size, patch_size, num_channels, depth], stddev=0.1))    # [5, 5, 1, 14]\n",
    "  layer1_biases = tf.Variable(tf.truncated_normal([depth]))\n",
    "  layer2_weights = tf.Variable(tf.truncated_normal(     \n",
    "      [patch_size, patch_size, depth, depth], stddev=0.1))           # [5, 5, 14, 28]  \n",
    "  layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))\n",
    "  layer3_weights = tf.Variable(tf.truncated_normal(\n",
    "      [image_size//4 * image_size//4 * depth, num_hidden], stddev=0.1))\n",
    "  layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
    "  layer4_weights = tf.Variable(tf.truncated_normal(\n",
    "      [num_hidden, num_labels], stddev=0.1))\n",
    "  layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  \n",
    "  # Model.\n",
    "  def model(data):\n",
    "    conv1 = tf.nn.conv2d(data, layer1_weights, [1, 1, 1, 1], padding='SAME')\n",
    "    hidden1 = tf.nn.relu(conv1 + layer1_biases)\n",
    "    pool1 = tf.nn.max_pool(hidden1, [1, 2, 2, 1], [1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "    conv2 = tf.nn.conv2d(pool1, layer2_weights, [1, 1, 1, 1], padding='SAME')\n",
    "    hidden2 = tf.nn.relu(conv2 + layer2_biases)\n",
    "    pool2 = tf.nn.max_pool(hidden2, [1, 2, 2, 1], [1, 2, 2, 1], padding='SAME')\n",
    "        \n",
    "    shape = pool2.get_shape().as_list()\n",
    "    reshape = tf.reshape(pool2, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "    \n",
    "    hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n",
    "    drop = tf.nn.dropout(hidden, 0.5)\n",
    "    \n",
    "    return tf.matmul(drop, layer4_weights) + layer4_biases\n",
    "\n",
    "  def modelTest(data):\n",
    "    conv1 = tf.nn.conv2d(data, layer1_weights, [1, 1, 1, 1], padding='SAME')\n",
    "    hidden1 = tf.nn.relu(conv1 + layer1_biases)\n",
    "    pool1 = tf.nn.max_pool(hidden1, [1, 2, 2, 1], [1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "    conv2 = tf.nn.conv2d(pool1, layer2_weights, [1, 1, 1, 1], padding='SAME')\n",
    "    hidden2 = tf.nn.relu(conv2 + layer2_biases)\n",
    "    pool2 = tf.nn.max_pool(hidden2, [1, 2, 2, 1], [1, 2, 2, 1], padding='SAME')\n",
    "    \n",
    "    shape = pool2.get_shape().as_list()\n",
    "    reshape = tf.reshape(pool2, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "    \n",
    "    hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n",
    "\n",
    "    return tf.matmul(hidden, layer4_weights) + layer4_biases\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = model(tf_train_dataset)\n",
    "  loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "    \n",
    "  # Optimizer.\n",
    "  #optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  optimizer = tf.train.AdamOptimizer().minimize(loss)\n",
    "\n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(modelTest(tf_valid_dataset))\n",
    "  test_prediction = tf.nn.softmax(modelTest(tf_test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 4.284821\n",
      "Minibatch accuracy: 12.5%\n",
      "Validation accuracy: 10.1%\n",
      "Minibatch loss at step 50: 2.345907\n",
      "Minibatch accuracy: 6.2%\n",
      "Validation accuracy: 10.6%\n",
      "Minibatch loss at step 100: 2.374837\n",
      "Minibatch accuracy: 6.2%\n",
      "Validation accuracy: 14.8%\n",
      "Minibatch loss at step 150: 2.110399\n",
      "Minibatch accuracy: 25.0%\n",
      "Validation accuracy: 15.8%\n",
      "Minibatch loss at step 200: 1.887661\n",
      "Minibatch accuracy: 37.5%\n",
      "Validation accuracy: 37.6%\n",
      "Minibatch loss at step 250: 1.955488\n",
      "Minibatch accuracy: 18.8%\n",
      "Validation accuracy: 53.2%\n",
      "Minibatch loss at step 300: 1.304958\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 56.6%\n",
      "Minibatch loss at step 350: 0.888772\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 59.4%\n",
      "Minibatch loss at step 400: 1.473631\n",
      "Minibatch accuracy: 50.0%\n",
      "Validation accuracy: 58.4%\n",
      "Minibatch loss at step 450: 1.329865\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 58.0%\n",
      "Minibatch loss at step 500: 1.472599\n",
      "Minibatch accuracy: 50.0%\n",
      "Validation accuracy: 59.1%\n",
      "Minibatch loss at step 550: 1.305393\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 60.1%\n",
      "Minibatch loss at step 600: 0.823692\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 62.2%\n",
      "Minibatch loss at step 650: 0.514150\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 62.6%\n",
      "Minibatch loss at step 700: 0.442917\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 63.4%\n",
      "Minibatch loss at step 750: 1.061771\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 62.9%\n",
      "Minibatch loss at step 800: 1.158240\n",
      "Minibatch accuracy: 56.2%\n",
      "Validation accuracy: 64.0%\n",
      "Minibatch loss at step 850: 0.589840\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 64.6%\n",
      "Minibatch loss at step 900: 0.309574\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 65.5%\n",
      "Minibatch loss at step 950: 0.172609\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 66.0%\n",
      "Minibatch loss at step 1000: 0.634478\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 66.0%\n",
      "Minibatch loss at step 1050: 0.577997\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 66.6%\n",
      "Minibatch loss at step 1100: 0.882922\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 66.7%\n",
      "Minibatch loss at step 1150: 0.863568\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 67.1%\n",
      "Minibatch loss at step 1200: 0.557338\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 67.9%\n",
      "Minibatch loss at step 1250: 0.318242\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 68.3%\n",
      "Minibatch loss at step 1300: 0.395321\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.9%\n",
      "Minibatch loss at step 1350: 0.839303\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 69.3%\n",
      "Minibatch loss at step 1400: 0.958529\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 68.6%\n",
      "Minibatch loss at step 1450: 0.688748\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 68.1%\n",
      "Minibatch loss at step 1500: 0.600934\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 69.2%\n",
      "Minibatch loss at step 1550: 1.020445\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 69.0%\n",
      "Minibatch loss at step 1600: 0.763868\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 70.0%\n",
      "Minibatch loss at step 1650: 0.512424\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 67.9%\n",
      "Minibatch loss at step 1700: 0.391175\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 69.1%\n",
      "Minibatch loss at step 1750: 0.416750\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 69.7%\n",
      "Minibatch loss at step 1800: 0.402115\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 69.7%\n",
      "Minibatch loss at step 1850: 0.959073\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 69.9%\n",
      "Minibatch loss at step 1900: 0.141099\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 70.7%\n",
      "Minibatch loss at step 1950: 0.241081\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 70.4%\n",
      "Minibatch loss at step 2000: 0.838398\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 70.8%\n",
      "Minibatch loss at step 2050: 0.677263\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 70.8%\n",
      "Minibatch loss at step 2100: 0.820343\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 71.5%\n",
      "Minibatch loss at step 2150: 0.220608\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 71.2%\n",
      "Minibatch loss at step 2200: 1.194195\n",
      "Minibatch accuracy: 56.2%\n",
      "Validation accuracy: 71.4%\n",
      "Minibatch loss at step 2250: 0.224710\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 72.1%\n",
      "Minibatch loss at step 2300: 0.988862\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 72.7%\n",
      "Minibatch loss at step 2350: 1.290620\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 72.2%\n",
      "Minibatch loss at step 2400: 0.496715\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 71.8%\n",
      "Minibatch loss at step 2450: 0.752158\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 72.4%\n",
      "Minibatch loss at step 2500: 0.131601\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 72.5%\n",
      "Minibatch loss at step 2550: 0.586498\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 72.3%\n",
      "Minibatch loss at step 2600: 0.275638\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 73.1%\n",
      "Minibatch loss at step 2650: 1.020582\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 71.6%\n",
      "Minibatch loss at step 2700: 0.937795\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 73.3%\n",
      "Minibatch loss at step 2750: 0.403380\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 73.0%\n",
      "Minibatch loss at step 2800: 0.798763\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 72.8%\n",
      "Minibatch loss at step 2850: 0.682165\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 70.8%\n",
      "Minibatch loss at step 2900: 0.740443\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 72.9%\n",
      "Minibatch loss at step 2950: 1.160336\n",
      "Minibatch accuracy: 50.0%\n",
      "Validation accuracy: 73.2%\n",
      "Minibatch loss at step 3000: 0.632870\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 73.6%\n",
      "Minibatch loss at step 3050: 0.563811\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 73.7%\n",
      "Minibatch loss at step 3100: 0.367769\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 73.2%\n",
      "Minibatch loss at step 3150: 0.531191\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 73.4%\n",
      "Minibatch loss at step 3200: 0.254041\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 73.7%\n",
      "Minibatch loss at step 3250: 0.794657\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 73.6%\n",
      "Minibatch loss at step 3300: 1.530949\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 74.3%\n",
      "Minibatch loss at step 3350: 0.804034\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 73.5%\n",
      "Minibatch loss at step 3400: 0.450467\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 74.6%\n",
      "Minibatch loss at step 3450: 0.415956\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 74.1%\n",
      "Minibatch loss at step 3500: 0.801769\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 73.2%\n",
      "Minibatch loss at step 3550: 1.057635\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 74.0%\n",
      "Minibatch loss at step 3600: 0.709283\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 74.2%\n",
      "Minibatch loss at step 3650: 0.515100\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 73.6%\n",
      "Minibatch loss at step 3700: 0.749514\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 74.1%\n",
      "Minibatch loss at step 3750: 0.552786\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 74.4%\n",
      "Minibatch loss at step 3800: 0.217009\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 74.0%\n",
      "Minibatch loss at step 3850: 0.649074\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 74.5%\n",
      "Minibatch loss at step 3900: 0.677811\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 74.2%\n",
      "Minibatch loss at step 3950: 0.137474\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 75.2%\n",
      "Minibatch loss at step 4000: 0.466154\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 74.3%\n",
      "Minibatch loss at step 4050: 0.354528\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 74.7%\n",
      "Minibatch loss at step 4100: 0.697458\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 75.2%\n",
      "Minibatch loss at step 4150: 0.273586\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 75.0%\n",
      "Minibatch loss at step 4200: 0.454249\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 74.9%\n",
      "Minibatch loss at step 4250: 0.838180\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 75.5%\n",
      "Minibatch loss at step 4300: 0.113582\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 75.2%\n",
      "Minibatch loss at step 4350: 0.886481\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 75.2%\n",
      "Minibatch loss at step 4400: 0.376369\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 75.0%\n",
      "Minibatch loss at step 4450: 0.874524\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 75.0%\n",
      "Minibatch loss at step 4500: 0.634052\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 74.8%\n",
      "Minibatch loss at step 4550: 0.554767\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 74.8%\n",
      "Minibatch loss at step 4600: 0.531386\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 74.7%\n",
      "Minibatch loss at step 4650: 0.135938\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 76.0%\n",
      "Minibatch loss at step 4700: 0.573616\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 76.6%\n",
      "Minibatch loss at step 4750: 0.201609\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 75.5%\n",
      "Minibatch loss at step 4800: 0.636770\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 75.9%\n",
      "Minibatch loss at step 4850: 0.253033\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 75.7%\n",
      "Minibatch loss at step 4900: 0.616201\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 76.3%\n",
      "Minibatch loss at step 4950: 0.323457\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 75.7%\n",
      "Minibatch loss at step 5000: 0.363636\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 75.3%\n",
      "Minibatch loss at step 5050: 0.448156\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 75.6%\n",
      "Minibatch loss at step 5100: 0.304154\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 76.0%\n",
      "Minibatch loss at step 5150: 0.860157\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 75.9%\n",
      "Minibatch loss at step 5200: 0.280862\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 75.2%\n",
      "Minibatch loss at step 5250: 0.588247\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 76.2%\n",
      "Minibatch loss at step 5300: 0.378825\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 76.4%\n",
      "Minibatch loss at step 5350: 0.590200\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 76.0%\n",
      "Minibatch loss at step 5400: 0.023847\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 75.2%\n",
      "Minibatch loss at step 5450: 0.608763\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 76.5%\n",
      "Minibatch loss at step 5500: 0.277753\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 76.2%\n",
      "Minibatch loss at step 5550: 0.707534\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 76.1%\n",
      "Minibatch loss at step 5600: 0.171248\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 75.4%\n",
      "Minibatch loss at step 5650: 0.213736\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 75.0%\n",
      "Minibatch loss at step 5700: 0.332701\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 75.8%\n",
      "Minibatch loss at step 5750: 0.404566\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 76.0%\n",
      "Minibatch loss at step 5800: 0.320264\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 76.6%\n",
      "Minibatch loss at step 5850: 0.524522\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 76.4%\n",
      "Minibatch loss at step 5900: 0.385453\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 76.2%\n",
      "Minibatch loss at step 5950: 0.994169\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 76.9%\n",
      "Minibatch loss at step 6000: 0.214902\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 76.4%\n",
      "Minibatch loss at step 6050: 0.385143\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 76.6%\n",
      "Minibatch loss at step 6100: 0.570140\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 75.9%\n",
      "Minibatch loss at step 6150: 0.827325\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 76.4%\n",
      "Minibatch loss at step 6200: 0.177182\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 76.1%\n",
      "Minibatch loss at step 6250: 0.582638\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 76.6%\n",
      "Minibatch loss at step 6300: 0.304126\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 76.3%\n",
      "Minibatch loss at step 6350: 0.342940\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 76.3%\n",
      "Minibatch loss at step 6400: 0.310146\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 75.3%\n",
      "Minibatch loss at step 6450: 0.372668\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 76.4%\n",
      "Minibatch loss at step 6500: 0.050352\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 76.3%\n",
      "Minibatch loss at step 6550: 0.338981\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 76.9%\n",
      "Minibatch loss at step 6600: 0.736851\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 76.5%\n",
      "Minibatch loss at step 6650: 0.556122\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 77.1%\n",
      "Minibatch loss at step 6700: 0.199698\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 76.7%\n",
      "Minibatch loss at step 6750: 0.096418\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 76.9%\n",
      "Minibatch loss at step 6800: 0.276864\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 75.8%\n",
      "Minibatch loss at step 6850: 0.247037\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 76.6%\n",
      "Minibatch loss at step 6900: 0.657098\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 76.5%\n",
      "Minibatch loss at step 6950: 0.201049\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 77.0%\n",
      "Minibatch loss at step 7000: 0.647341\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 76.8%\n",
      "Minibatch loss at step 7050: 0.310541\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 77.2%\n",
      "Minibatch loss at step 7100: 0.553753\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 77.0%\n",
      "Minibatch loss at step 7150: 0.296592\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 76.0%\n",
      "Minibatch loss at step 7200: 0.245412\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 76.8%\n",
      "Minibatch loss at step 7250: 0.038846\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 76.0%\n",
      "Minibatch loss at step 7300: 0.264488\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 77.2%\n",
      "Minibatch loss at step 7350: 0.681461\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 77.0%\n",
      "Minibatch loss at step 7400: 0.162398\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 76.7%\n",
      "Minibatch loss at step 7450: 0.177896\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 77.1%\n",
      "Minibatch loss at step 7500: 0.485206\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 76.9%\n",
      "Minibatch loss at step 7550: 0.691031\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 76.8%\n",
      "Minibatch loss at step 7600: 0.413562\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 76.8%\n",
      "Minibatch loss at step 7650: 0.670248\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 77.0%\n",
      "Minibatch loss at step 7700: 0.459452\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 77.2%\n",
      "Minibatch loss at step 7750: 0.552856\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 77.3%\n",
      "Minibatch loss at step 7800: 0.917940\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 77.0%\n",
      "Minibatch loss at step 7850: 0.419671\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 76.7%\n",
      "Minibatch loss at step 7900: 0.295532\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 76.9%\n",
      "Minibatch loss at step 7950: 0.292986\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 77.4%\n",
      "Minibatch loss at step 8000: 0.212673\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 76.6%\n",
      "Minibatch loss at step 8050: 0.524352\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 77.0%\n",
      "Minibatch loss at step 8100: 0.291614\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 77.4%\n",
      "Minibatch loss at step 8150: 0.705801\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 77.3%\n",
      "Minibatch loss at step 8200: 0.155508\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 77.4%\n",
      "Minibatch loss at step 8250: 0.447689\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 77.5%\n",
      "Minibatch loss at step 8300: 0.072960\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 77.4%\n",
      "Minibatch loss at step 8350: 0.362446\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 77.2%\n",
      "Minibatch loss at step 8400: 0.896421\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 77.6%\n",
      "Minibatch loss at step 8450: 0.073508\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 77.6%\n",
      "Minibatch loss at step 8500: 0.761959\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 77.4%\n",
      "Minibatch loss at step 8550: 0.337848\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 76.8%\n",
      "Minibatch loss at step 8600: 0.379435\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 77.2%\n",
      "Minibatch loss at step 8650: 0.352877\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 77.4%\n",
      "Minibatch loss at step 8700: 0.188328\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 77.5%\n",
      "Minibatch loss at step 8750: 0.357776\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 77.2%\n",
      "Minibatch loss at step 8800: 0.747609\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 77.1%\n",
      "Minibatch loss at step 8850: 0.422560\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 77.8%\n",
      "Minibatch loss at step 8900: 0.012026\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 77.5%\n",
      "Minibatch loss at step 8950: 0.352592\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 77.2%\n",
      "Minibatch loss at step 9000: 0.279572\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 77.2%\n",
      "Minibatch loss at step 9050: 0.526097\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 77.0%\n",
      "Minibatch loss at step 9100: 0.301440\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 76.8%\n",
      "Minibatch loss at step 9150: 0.495733\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 77.3%\n",
      "Minibatch loss at step 9200: 0.501882\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 77.0%\n",
      "Minibatch loss at step 9250: 0.123580\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 77.3%\n",
      "Minibatch loss at step 9300: 0.379166\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 77.0%\n",
      "Minibatch loss at step 9350: 0.180619\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 76.8%\n",
      "Minibatch loss at step 9400: 0.269579\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 76.5%\n",
      "Minibatch loss at step 9450: 0.360664\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 76.7%\n",
      "Minibatch loss at step 9500: 0.176410\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 76.8%\n",
      "Minibatch loss at step 9550: 0.199882\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 77.2%\n",
      "Minibatch loss at step 9600: 0.351137\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 77.4%\n",
      "Minibatch loss at step 9650: 0.315832\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 77.6%\n",
      "Minibatch loss at step 9700: 0.138719\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 77.9%\n",
      "Minibatch loss at step 9750: 0.102592\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 77.7%\n",
      "Minibatch loss at step 9800: 0.525702\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 77.6%\n",
      "Minibatch loss at step 9850: 0.383277\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 77.8%\n",
      "Minibatch loss at step 9900: 0.381485\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 77.4%\n",
      "Minibatch loss at step 9950: 0.529437\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 77.4%\n",
      "Minibatch loss at step 10000: 0.513881\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 77.0%\n",
      "Minibatch loss at step 10050: 0.450923\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 77.2%\n",
      "Minibatch loss at step 10100: 0.318374\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 77.9%\n",
      "Minibatch loss at step 10150: 1.001071\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 77.1%\n",
      "Minibatch loss at step 10200: 0.228453\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 77.7%\n",
      "Minibatch loss at step 10250: 0.648273\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 77.6%\n",
      "Minibatch loss at step 10300: 0.351884\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 77.7%\n",
      "Minibatch loss at step 10350: 0.138738\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 77.5%\n",
      "Minibatch loss at step 10400: 0.458489\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 77.1%\n",
      "Minibatch loss at step 10450: 0.482978\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 78.0%\n",
      "Minibatch loss at step 10500: 0.331161\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 77.1%\n",
      "Minibatch loss at step 10550: 0.230761\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 77.7%\n",
      "Minibatch loss at step 10600: 0.344580\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 77.7%\n",
      "Minibatch loss at step 10650: 0.519489\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 76.8%\n",
      "Minibatch loss at step 10700: 0.356684\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 77.3%\n",
      "Minibatch loss at step 10750: 0.519247\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 77.4%\n",
      "Minibatch loss at step 10800: 0.172497\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 77.6%\n",
      "Minibatch loss at step 10850: 0.840944\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 77.2%\n",
      "Minibatch loss at step 10900: 0.228273\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 77.5%\n",
      "Minibatch loss at step 10950: 0.211620\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 77.0%\n",
      "Minibatch loss at step 11000: 0.570458\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 77.6%\n",
      "Minibatch loss at step 11050: 0.451663\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 77.7%\n",
      "Minibatch loss at step 11100: 0.604431\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 77.7%\n",
      "Minibatch loss at step 11150: 0.255220\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 78.0%\n",
      "Minibatch loss at step 11200: 0.089596\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 77.6%\n",
      "Minibatch loss at step 11250: 0.860653\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 77.7%\n",
      "Minibatch loss at step 11300: 0.268392\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 77.2%\n",
      "Minibatch loss at step 11350: 0.292768\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 77.5%\n",
      "Minibatch loss at step 11400: 0.076264\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 77.0%\n",
      "Minibatch loss at step 11450: 0.402206\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 77.3%\n",
      "Minibatch loss at step 11500: 0.212694\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 78.0%\n",
      "Minibatch loss at step 11550: 0.506204\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 77.5%\n",
      "Minibatch loss at step 11600: 0.329913\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 77.6%\n",
      "Minibatch loss at step 11650: 0.624224\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 77.6%\n",
      "Minibatch loss at step 11700: 0.124717\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 78.1%\n",
      "Minibatch loss at step 11750: 0.438097\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 78.1%\n",
      "Minibatch loss at step 11800: 0.568639\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 77.9%\n",
      "Minibatch loss at step 11850: 0.222825\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 77.7%\n",
      "Minibatch loss at step 11900: 0.666161\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 78.0%\n",
      "Minibatch loss at step 11950: 0.303837\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 77.6%\n",
      "Minibatch loss at step 12000: 0.500540\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 77.0%\n",
      "Minibatch loss at step 12050: 0.661688\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 77.9%\n",
      "Minibatch loss at step 12100: 0.246369\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 77.8%\n",
      "Minibatch loss at step 12150: 0.353682\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 78.1%\n",
      "Minibatch loss at step 12200: 0.700330\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 78.1%\n",
      "Minibatch loss at step 12250: 0.779587\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 77.9%\n",
      "Minibatch loss at step 12300: 0.467271\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 77.8%\n",
      "Minibatch loss at step 12350: 0.420031\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 76.8%\n",
      "Minibatch loss at step 12400: 0.105574\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 78.1%\n",
      "Minibatch loss at step 12450: 0.426144\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 78.3%\n",
      "Minibatch loss at step 12500: 0.478115\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 78.0%\n",
      "Minibatch loss at step 12550: 0.459530\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 78.4%\n",
      "Minibatch loss at step 12600: 0.026689\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 77.8%\n",
      "Minibatch loss at step 12650: 0.679118\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 78.5%\n",
      "Minibatch loss at step 12700: 0.077805\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 78.3%\n",
      "Minibatch loss at step 12750: 0.361591\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 77.2%\n",
      "Minibatch loss at step 12800: 0.867062\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 77.4%\n",
      "Minibatch loss at step 12850: 0.406758\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 77.5%\n",
      "Minibatch loss at step 12900: 0.626748\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 77.7%\n",
      "Minibatch loss at step 12950: 0.551641\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 77.8%\n",
      "Minibatch loss at step 13000: 0.416599\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 77.7%\n",
      "Minibatch loss at step 13050: 0.249331\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 78.6%\n",
      "Minibatch loss at step 13100: 0.340842\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 78.4%\n",
      "Minibatch loss at step 13150: 0.346652\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 78.4%\n",
      "Minibatch loss at step 13200: 0.205321\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 78.2%\n",
      "Minibatch loss at step 13250: 0.132087\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 78.5%\n",
      "Minibatch loss at step 13300: 0.507025\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 78.2%\n",
      "Minibatch loss at step 13350: 0.195455\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 78.2%\n",
      "Minibatch loss at step 13400: 0.677465\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 77.2%\n",
      "Minibatch loss at step 13450: 0.004559\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 77.7%\n",
      "Minibatch loss at step 13500: 1.187370\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 77.9%\n",
      "Minibatch loss at step 13550: 0.788346\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 77.6%\n",
      "Minibatch loss at step 13600: 0.440369\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 78.2%\n",
      "Minibatch loss at step 13650: 0.018598\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 78.0%\n",
      "Minibatch loss at step 13700: 0.154649\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 78.4%\n",
      "Minibatch loss at step 13750: 0.533852\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 78.3%\n",
      "Minibatch loss at step 13800: 0.523017\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 78.1%\n",
      "Minibatch loss at step 13850: 0.479657\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 78.0%\n",
      "Minibatch loss at step 13900: 0.007890\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 77.9%\n",
      "Minibatch loss at step 13950: 0.144498\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 77.8%\n",
      "Minibatch loss at step 14000: 0.271547\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 77.6%\n",
      "Minibatch loss at step 14050: 0.198473\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 77.6%\n",
      "Minibatch loss at step 14100: 0.968172\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 77.8%\n",
      "Minibatch loss at step 14150: 0.486984\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 78.1%\n",
      "Minibatch loss at step 14200: 0.500916\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 77.9%\n",
      "Minibatch loss at step 14250: 0.250593\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 77.5%\n",
      "Minibatch loss at step 14300: 0.166194\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 78.2%\n",
      "Minibatch loss at step 14350: 0.370288\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 77.8%\n",
      "Minibatch loss at step 14400: 0.234045\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 78.6%\n",
      "Minibatch loss at step 14450: 0.003451\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 78.3%\n",
      "Minibatch loss at step 14500: 0.406255\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 78.0%\n",
      "Minibatch loss at step 14550: 0.390621\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 78.4%\n",
      "Minibatch loss at step 14600: 0.644792\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 78.3%\n",
      "Minibatch loss at step 14650: 0.635140\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 78.2%\n",
      "Minibatch loss at step 14700: 0.628894\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 78.0%\n",
      "Minibatch loss at step 14750: 0.490794\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 78.7%\n",
      "Minibatch loss at step 14800: 0.267918\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 78.0%\n",
      "Minibatch loss at step 14850: 0.043751\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 78.8%\n",
      "Minibatch loss at step 14900: 0.299764\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 78.5%\n",
      "Minibatch loss at step 14950: 0.033531\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 78.4%\n",
      "Minibatch loss at step 15000: 0.229801\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 78.1%\n",
      "Test accuracy: 94.6%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 15001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  for step in range(num_steps):\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 50 == 0):\n",
    "      print('Minibatch loss at step %d: %f' % (step, l))\n",
    "      print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "      print('Validation accuracy: %.1f%%' % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "patch_size = 5\n",
    "depth = 14\n",
    "num_hidden = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  learning_rate = tf.placeholder(tf.float32)\n",
    "  \n",
    "  # Variables.\n",
    "  layer1_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size, patch_size, num_channels, depth], stddev=0.1))    # [5, 5, 1, 14]\n",
    "  layer1_biases = tf.Variable(tf.truncated_normal([depth]))\n",
    "  layer2_weights = tf.Variable(tf.truncated_normal(     \n",
    "      [patch_size, patch_size, depth, depth], stddev=0.1))           # [5, 5, 14, 28]  \n",
    "  layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))\n",
    "  layer3_weights = tf.Variable(tf.truncated_normal(\n",
    "      [image_size//4 * image_size//4 * depth, num_hidden], stddev=0.1))\n",
    "  layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
    "  layer4_weights = tf.Variable(tf.truncated_normal(\n",
    "      [num_hidden, num_labels], stddev=0.1))\n",
    "  layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  \n",
    "  # Model.\n",
    "  def model(data):\n",
    "    conv1 = tf.nn.conv2d(data, layer1_weights, [1, 1, 1, 1], padding='SAME')\n",
    "    hidden1 = tf.nn.relu(conv1 + layer1_biases)\n",
    "    pool1 = tf.nn.max_pool(hidden1, [1, 2, 2, 1], [1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "    conv2 = tf.nn.conv2d(pool1, layer2_weights, [1, 1, 1, 1], padding='SAME')\n",
    "    hidden2 = tf.nn.relu(conv2 + layer2_biases)\n",
    "    pool2 = tf.nn.max_pool(hidden2, [1, 2, 2, 1], [1, 2, 2, 1], padding='SAME')\n",
    "        \n",
    "    shape = pool2.get_shape().as_list()\n",
    "    reshape = tf.reshape(pool2, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "    \n",
    "    hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n",
    "    drop = tf.nn.dropout(hidden, 0.5)\n",
    "    \n",
    "    return tf.matmul(drop, layer4_weights) + layer4_biases\n",
    "\n",
    "  def modelTest(data):\n",
    "    conv1 = tf.nn.conv2d(data, layer1_weights, [1, 1, 1, 1], padding='SAME')\n",
    "    hidden1 = tf.nn.relu(conv1 + layer1_biases)\n",
    "    pool1 = tf.nn.max_pool(hidden1, [1, 2, 2, 1], [1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "    conv2 = tf.nn.conv2d(pool1, layer2_weights, [1, 1, 1, 1], padding='SAME')\n",
    "    hidden2 = tf.nn.relu(conv2 + layer2_biases)\n",
    "    pool2 = tf.nn.max_pool(hidden2, [1, 2, 2, 1], [1, 2, 2, 1], padding='SAME')\n",
    "    \n",
    "    shape = pool2.get_shape().as_list()\n",
    "    reshape = tf.reshape(pool2, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "    \n",
    "    hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n",
    "\n",
    "    return tf.matmul(hidden, layer4_weights) + layer4_biases\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = model(tf_train_dataset)\n",
    "  loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "    \n",
    "  # Optimizer.\n",
    "  #optimizer = tf.train.AdamOptimizer().minimize(loss)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(modelTest(tf_valid_dataset))\n",
    "  test_prediction = tf.nn.softmax(modelTest(tf_test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001  completed\n",
      "0.005  completed\n",
      "0.01  completed\n",
      "0.05  completed\n",
      "0.1  completed\n",
      "0.5  completed\n"
     ]
    }
   ],
   "source": [
    "num_steps = 1001\n",
    "learning_rates = [0.001, 0.005, 0.01, 0.05, 0.1, 0.5]\n",
    "accuracy_test = []\n",
    "accuracy_valid = []\n",
    "\n",
    "for rate in learning_rates:\n",
    "    with tf.Session(graph=graph) as session:\n",
    "      tf.global_variables_initializer().run()\n",
    "      valid_sum = []\n",
    "      for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, learning_rate : rate}\n",
    "        _, l, predictions = session.run(\n",
    "          [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 50 == 0):\n",
    "           valid_sum.append(accuracy(valid_prediction.eval(), valid_labels))\n",
    "      accuracy_valid.append(np.mean(valid_sum))\n",
    "      accuracy_test.append(accuracy(test_prediction.eval(), test_labels))\n",
    "    print(rate, \" completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEMCAYAAADUEk3/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc3164c5f28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.semilogx(learning_rates, accuracy_valid)\n",
    "plt.grid(True)\n",
    "plt.title('Accuracy valid')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEMCAYAAADUEk3/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc3164c57b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.semilogx(learning_rates, accuracy_test)\n",
    "plt.grid(True)\n",
    "plt.title('Accuracy Test')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 4.184390\n",
      "Minibatch accuracy: 25.0%\n",
      "Validation accuracy: 10.0%\n",
      "Minibatch loss at step 50: 1.846732\n",
      "Minibatch accuracy: 31.2%\n",
      "Validation accuracy: 22.3%\n",
      "Minibatch loss at step 100: 1.339309\n",
      "Minibatch accuracy: 56.2%\n",
      "Validation accuracy: 45.6%\n",
      "Minibatch loss at step 150: 1.284660\n",
      "Minibatch accuracy: 50.0%\n",
      "Validation accuracy: 53.6%\n",
      "Minibatch loss at step 200: 0.980369\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 57.6%\n",
      "Minibatch loss at step 250: 1.110217\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 56.5%\n",
      "Minibatch loss at step 300: 1.378198\n",
      "Minibatch accuracy: 50.0%\n",
      "Validation accuracy: 53.7%\n",
      "Minibatch loss at step 350: 0.678970\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 58.9%\n",
      "Minibatch loss at step 400: 1.147608\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 61.3%\n",
      "Minibatch loss at step 450: 0.682837\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 60.5%\n",
      "Minibatch loss at step 500: 0.966087\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 63.3%\n",
      "Minibatch loss at step 550: 1.078412\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 61.5%\n",
      "Minibatch loss at step 600: 0.913244\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 61.1%\n",
      "Minibatch loss at step 650: 0.784419\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 64.7%\n",
      "Minibatch loss at step 700: 0.429956\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 63.7%\n",
      "Minibatch loss at step 750: 0.604991\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 63.1%\n",
      "Minibatch loss at step 800: 1.312212\n",
      "Minibatch accuracy: 56.2%\n",
      "Validation accuracy: 64.7%\n",
      "Minibatch loss at step 850: 0.360249\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 64.4%\n",
      "Minibatch loss at step 900: 1.184190\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 55.9%\n",
      "Minibatch loss at step 950: 0.022035\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 65.0%\n",
      "Minibatch loss at step 1000: 0.575827\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 62.9%\n",
      "Minibatch loss at step 1050: 0.558310\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 62.7%\n",
      "Minibatch loss at step 1100: 1.645809\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 64.8%\n",
      "Minibatch loss at step 1150: 1.076555\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 64.9%\n",
      "Minibatch loss at step 1200: 0.340011\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 66.8%\n",
      "Minibatch loss at step 1250: 0.596412\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 65.0%\n",
      "Minibatch loss at step 1300: 0.219468\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 64.8%\n",
      "Minibatch loss at step 1350: 0.547186\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 67.2%\n",
      "Minibatch loss at step 1400: 0.888405\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 63.5%\n",
      "Minibatch loss at step 1450: 0.830887\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 64.7%\n",
      "Minibatch loss at step 1500: 0.822128\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 63.1%\n",
      "Minibatch loss at step 1550: 0.833625\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 65.1%\n",
      "Minibatch loss at step 1600: 0.492214\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 66.0%\n",
      "Minibatch loss at step 1650: 0.550764\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 65.5%\n",
      "Minibatch loss at step 1700: 0.261494\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 67.3%\n",
      "Minibatch loss at step 1750: 0.320483\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 65.0%\n",
      "Minibatch loss at step 1800: 0.534951\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 66.6%\n",
      "Minibatch loss at step 1850: 0.760933\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 67.9%\n",
      "Minibatch loss at step 1900: 0.077708\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 67.3%\n",
      "Minibatch loss at step 1950: 0.665237\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 66.4%\n",
      "Minibatch loss at step 2000: 1.060957\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 65.1%\n",
      "Minibatch loss at step 2050: 0.735304\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 68.4%\n",
      "Minibatch loss at step 2100: 1.242761\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 68.2%\n",
      "Minibatch loss at step 2150: 0.506842\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 69.1%\n",
      "Minibatch loss at step 2200: 1.044451\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 68.2%\n",
      "Minibatch loss at step 2250: 0.655446\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 67.9%\n",
      "Minibatch loss at step 2300: 0.854184\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 64.7%\n",
      "Minibatch loss at step 2350: 0.874824\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 69.2%\n",
      "Minibatch loss at step 2400: 0.407149\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 69.5%\n",
      "Minibatch loss at step 2450: 0.768316\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 69.9%\n",
      "Minibatch loss at step 2500: 0.300970\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 68.6%\n",
      "Minibatch loss at step 2550: 0.583270\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 66.7%\n",
      "Minibatch loss at step 2600: 0.569211\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 66.6%\n",
      "Minibatch loss at step 2650: 0.879714\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 68.0%\n",
      "Minibatch loss at step 2700: 0.530885\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 68.3%\n",
      "Minibatch loss at step 2750: 0.533385\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 69.6%\n",
      "Minibatch loss at step 2800: 0.846787\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 67.0%\n",
      "Minibatch loss at step 2850: 1.547485\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 68.9%\n",
      "Minibatch loss at step 2900: 0.799773\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 69.0%\n",
      "Minibatch loss at step 2950: 1.245824\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 67.3%\n",
      "Minibatch loss at step 3000: 0.762415\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 66.3%\n",
      "Minibatch loss at step 3050: 0.574385\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 68.6%\n",
      "Minibatch loss at step 3100: 0.564896\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 69.4%\n",
      "Minibatch loss at step 3150: 0.640688\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 69.7%\n",
      "Minibatch loss at step 3200: 0.087149\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 69.9%\n",
      "Minibatch loss at step 3250: 0.946160\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 68.3%\n",
      "Minibatch loss at step 3300: 0.971402\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 68.6%\n",
      "Minibatch loss at step 3350: 1.243959\n",
      "Minibatch accuracy: 56.2%\n",
      "Validation accuracy: 69.6%\n",
      "Minibatch loss at step 3400: 1.199644\n",
      "Minibatch accuracy: 56.2%\n",
      "Validation accuracy: 59.2%\n",
      "Minibatch loss at step 3450: 0.766183\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 70.2%\n",
      "Minibatch loss at step 3500: 0.897437\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 68.5%\n",
      "Minibatch loss at step 3550: 1.366127\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 69.5%\n",
      "Minibatch loss at step 3600: 0.650321\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 69.9%\n",
      "Minibatch loss at step 3650: 0.674523\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 68.3%\n",
      "Minibatch loss at step 3700: 0.766549\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 70.2%\n",
      "Minibatch loss at step 3750: 0.668213\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 70.5%\n",
      "Minibatch loss at step 3800: 0.077950\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 69.8%\n",
      "Minibatch loss at step 3850: 0.625768\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 70.4%\n",
      "Minibatch loss at step 3900: 0.494439\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 67.2%\n",
      "Minibatch loss at step 3950: 0.150125\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 69.5%\n",
      "Minibatch loss at step 4000: 0.537847\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 70.4%\n",
      "Minibatch loss at step 4050: 0.464240\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 71.1%\n",
      "Minibatch loss at step 4100: 0.635375\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 69.7%\n",
      "Minibatch loss at step 4150: 0.284664\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 70.8%\n",
      "Minibatch loss at step 4200: 0.452965\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 71.2%\n",
      "Minibatch loss at step 4250: 0.915339\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 70.2%\n",
      "Minibatch loss at step 4300: 0.136900\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 68.5%\n",
      "Minibatch loss at step 4350: 1.909028\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 71.9%\n",
      "Minibatch loss at step 4400: 0.159033\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 70.8%\n",
      "Minibatch loss at step 4450: 0.820381\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 70.9%\n",
      "Minibatch loss at step 4500: 0.606590\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 69.7%\n",
      "Minibatch loss at step 4550: 0.681987\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 71.2%\n",
      "Minibatch loss at step 4600: 1.038063\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 71.3%\n",
      "Minibatch loss at step 4650: 0.133647\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 71.1%\n",
      "Minibatch loss at step 4700: 0.442016\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 71.5%\n",
      "Minibatch loss at step 4750: 0.263355\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 71.6%\n",
      "Minibatch loss at step 4800: 0.564781\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 69.0%\n",
      "Minibatch loss at step 4850: 0.303744\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 70.9%\n",
      "Minibatch loss at step 4900: 0.689481\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 72.5%\n",
      "Minibatch loss at step 4950: 0.466231\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 70.0%\n",
      "Minibatch loss at step 5000: 0.292593\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 71.9%\n",
      "Minibatch loss at step 5050: 0.688166\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 70.6%\n",
      "Minibatch loss at step 5100: 0.424263\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 71.9%\n",
      "Minibatch loss at step 5150: 0.847794\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 71.7%\n",
      "Minibatch loss at step 5200: 0.300286\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 71.8%\n",
      "Minibatch loss at step 5250: 0.666394\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 71.7%\n",
      "Minibatch loss at step 5300: 0.486998\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 71.9%\n",
      "Minibatch loss at step 5350: 1.261026\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 72.0%\n",
      "Minibatch loss at step 5400: 0.266274\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 68.7%\n",
      "Minibatch loss at step 5450: 1.039200\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 71.9%\n",
      "Minibatch loss at step 5500: 0.163473\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 73.3%\n",
      "Minibatch loss at step 5550: 0.580033\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 71.6%\n",
      "Minibatch loss at step 5600: 0.286196\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 71.3%\n",
      "Minibatch loss at step 5650: 0.153128\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 72.5%\n",
      "Minibatch loss at step 5700: 0.429754\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 72.7%\n",
      "Minibatch loss at step 5750: 0.621179\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 71.3%\n",
      "Minibatch loss at step 5800: 0.353445\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 72.3%\n",
      "Minibatch loss at step 5850: 0.660900\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 72.6%\n",
      "Minibatch loss at step 5900: 0.278968\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 71.1%\n",
      "Minibatch loss at step 5950: 1.015387\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 73.8%\n",
      "Minibatch loss at step 6000: 0.286066\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 69.7%\n",
      "Minibatch loss at step 6050: 0.458667\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 73.3%\n",
      "Minibatch loss at step 6100: 0.793683\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 73.1%\n",
      "Minibatch loss at step 6150: 0.402872\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 72.3%\n",
      "Minibatch loss at step 6200: 0.317889\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 70.0%\n",
      "Minibatch loss at step 6250: 1.243876\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 73.1%\n",
      "Minibatch loss at step 6300: 0.191606\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 71.6%\n",
      "Minibatch loss at step 6350: 0.398500\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 71.8%\n",
      "Minibatch loss at step 6400: 1.031798\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 72.2%\n",
      "Minibatch loss at step 6450: 0.491908\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 73.4%\n",
      "Minibatch loss at step 6500: 0.214213\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 70.3%\n",
      "Minibatch loss at step 6550: 0.300301\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 71.0%\n",
      "Minibatch loss at step 6600: 0.663722\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 73.3%\n",
      "Minibatch loss at step 6650: 0.428230\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 73.1%\n",
      "Minibatch loss at step 6700: 0.277837\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 70.4%\n",
      "Minibatch loss at step 6750: 0.247482\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 71.5%\n",
      "Minibatch loss at step 6800: 0.401239\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 72.6%\n",
      "Minibatch loss at step 6850: 0.090517\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 73.6%\n",
      "Minibatch loss at step 6900: 0.567656\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 73.3%\n",
      "Minibatch loss at step 6950: 0.217075\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 73.3%\n",
      "Minibatch loss at step 7000: 0.665286\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 72.7%\n",
      "Minibatch loss at step 7050: 0.798993\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 73.6%\n",
      "Minibatch loss at step 7100: 0.712210\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 73.4%\n",
      "Minibatch loss at step 7150: 0.262666\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 72.7%\n",
      "Minibatch loss at step 7200: 0.346561\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 73.3%\n",
      "Minibatch loss at step 7250: 0.389939\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 71.9%\n",
      "Minibatch loss at step 7300: 0.266779\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 73.0%\n",
      "Minibatch loss at step 7350: 0.424174\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 72.3%\n",
      "Minibatch loss at step 7400: 0.229121\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 74.0%\n",
      "Minibatch loss at step 7450: 0.289882\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 73.1%\n",
      "Minibatch loss at step 7500: 0.859203\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 73.5%\n",
      "Minibatch loss at step 7550: 0.596615\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 70.7%\n",
      "Minibatch loss at step 7600: 0.564270\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 74.2%\n",
      "Minibatch loss at step 7650: 0.524540\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 74.4%\n",
      "Minibatch loss at step 7700: 0.437958\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 73.8%\n",
      "Minibatch loss at step 7750: 0.409241\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 74.7%\n",
      "Minibatch loss at step 7800: 1.053034\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 73.2%\n",
      "Minibatch loss at step 7850: 0.554110\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 74.6%\n",
      "Minibatch loss at step 7900: 0.574497\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 70.8%\n",
      "Minibatch loss at step 7950: 0.253387\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 73.7%\n",
      "Minibatch loss at step 8000: 0.321297\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 73.0%\n",
      "Minibatch loss at step 8050: 0.976912\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 71.3%\n",
      "Minibatch loss at step 8100: 0.322918\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 72.8%\n",
      "Minibatch loss at step 8150: 0.442712\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 72.6%\n",
      "Minibatch loss at step 8200: 0.174209\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 71.5%\n",
      "Minibatch loss at step 8250: 0.551115\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 73.8%\n",
      "Minibatch loss at step 8300: 0.380275\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 74.4%\n",
      "Minibatch loss at step 8350: 0.444378\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 73.1%\n",
      "Minibatch loss at step 8400: 0.467061\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 72.1%\n",
      "Minibatch loss at step 8450: 0.109571\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 73.8%\n",
      "Minibatch loss at step 8500: 0.601960\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 69.8%\n",
      "Minibatch loss at step 8550: 0.236405\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 72.6%\n",
      "Minibatch loss at step 8600: 0.469325\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 70.8%\n",
      "Minibatch loss at step 8650: 0.451416\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 73.2%\n",
      "Minibatch loss at step 8700: 0.303939\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 72.8%\n",
      "Minibatch loss at step 8750: 0.334122\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 72.5%\n",
      "Minibatch loss at step 8800: 0.937480\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 71.1%\n",
      "Minibatch loss at step 8850: 0.339301\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 72.8%\n",
      "Minibatch loss at step 8900: 0.048305\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 73.4%\n",
      "Minibatch loss at step 8950: 0.488750\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 73.0%\n",
      "Minibatch loss at step 9000: 0.579857\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 72.4%\n",
      "Minibatch loss at step 9050: 0.529778\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 74.2%\n",
      "Minibatch loss at step 9100: 0.420793\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 74.0%\n",
      "Minibatch loss at step 9150: 0.487335\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 74.2%\n",
      "Minibatch loss at step 9200: 1.009483\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 74.2%\n",
      "Minibatch loss at step 9250: 0.040259\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 74.7%\n",
      "Minibatch loss at step 9300: 0.417888\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 73.5%\n",
      "Minibatch loss at step 9350: 0.261336\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 74.3%\n",
      "Minibatch loss at step 9400: 0.253974\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 71.7%\n",
      "Minibatch loss at step 9450: 0.381082\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 73.8%\n",
      "Minibatch loss at step 9500: 0.371868\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 74.9%\n",
      "Minibatch loss at step 9550: 0.468068\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 73.5%\n",
      "Minibatch loss at step 9600: 0.279685\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 71.2%\n",
      "Minibatch loss at step 9650: 0.164375\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 73.3%\n",
      "Minibatch loss at step 9700: 0.092770\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 73.8%\n",
      "Minibatch loss at step 9750: 0.337770\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 72.7%\n",
      "Minibatch loss at step 9800: 0.509285\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 71.3%\n",
      "Minibatch loss at step 9850: 0.953682\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 73.4%\n",
      "Minibatch loss at step 9900: 0.292430\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 72.6%\n",
      "Minibatch loss at step 9950: 0.657653\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 73.8%\n",
      "Minibatch loss at step 10000: 0.491908\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 73.7%\n",
      "Minibatch loss at step 10050: 0.526161\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 74.2%\n",
      "Minibatch loss at step 10100: 0.500394\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 73.3%\n",
      "Minibatch loss at step 10150: 1.401290\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 74.4%\n",
      "Minibatch loss at step 10200: 0.168468\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 74.3%\n",
      "Minibatch loss at step 10250: 0.628079\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 74.7%\n",
      "Minibatch loss at step 10300: 0.186040\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 74.4%\n",
      "Minibatch loss at step 10350: 0.123567\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 73.7%\n",
      "Minibatch loss at step 10400: 0.276128\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 73.7%\n",
      "Minibatch loss at step 10450: 0.470083\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 72.8%\n",
      "Minibatch loss at step 10500: 0.620749\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 73.2%\n",
      "Minibatch loss at step 10550: 0.366917\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 74.2%\n",
      "Minibatch loss at step 10600: 0.447614\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 73.4%\n",
      "Minibatch loss at step 10650: 0.686333\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 72.2%\n",
      "Minibatch loss at step 10700: 0.415230\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 72.0%\n",
      "Minibatch loss at step 10750: 0.601964\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 73.7%\n",
      "Minibatch loss at step 10800: 0.211728\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 74.3%\n",
      "Minibatch loss at step 10850: 0.434627\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 74.0%\n",
      "Minibatch loss at step 10900: 0.135165\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 74.8%\n",
      "Minibatch loss at step 10950: 0.359957\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 73.4%\n",
      "Minibatch loss at step 11000: 0.449494\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 74.4%\n",
      "Minibatch loss at step 11050: 0.581361\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 74.7%\n",
      "Minibatch loss at step 11100: 0.705247\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 74.8%\n",
      "Minibatch loss at step 11150: 0.317466\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 74.7%\n",
      "Minibatch loss at step 11200: 0.143487\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 74.5%\n",
      "Minibatch loss at step 11250: 0.727411\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 71.7%\n",
      "Minibatch loss at step 11300: 0.225814\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 73.9%\n",
      "Minibatch loss at step 11350: 0.313302\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 75.1%\n",
      "Minibatch loss at step 11400: 0.090203\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 73.8%\n",
      "Minibatch loss at step 11450: 0.658030\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 74.6%\n",
      "Minibatch loss at step 11500: 0.400708\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 73.7%\n",
      "Minibatch loss at step 11550: 0.800911\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 74.9%\n",
      "Minibatch loss at step 11600: 0.261879\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 73.3%\n",
      "Minibatch loss at step 11650: 0.835568\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 74.1%\n",
      "Minibatch loss at step 11700: 0.123682\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 74.2%\n",
      "Minibatch loss at step 11750: 0.404936\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 74.5%\n",
      "Minibatch loss at step 11800: 0.644029\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 74.9%\n",
      "Minibatch loss at step 11850: 0.354191\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 71.8%\n",
      "Minibatch loss at step 11900: 0.760885\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 74.5%\n",
      "Minibatch loss at step 11950: 0.182174\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 74.2%\n",
      "Minibatch loss at step 12000: 0.777640\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 72.5%\n",
      "Minibatch loss at step 12050: 0.576680\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 73.7%\n",
      "Minibatch loss at step 12100: 0.521475\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 73.3%\n",
      "Minibatch loss at step 12150: 0.435803\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 73.6%\n",
      "Minibatch loss at step 12200: 0.512613\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 73.5%\n",
      "Minibatch loss at step 12250: 0.874099\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 74.1%\n",
      "Minibatch loss at step 12300: 0.476829\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 74.4%\n",
      "Minibatch loss at step 12350: 0.437097\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 73.8%\n",
      "Minibatch loss at step 12400: 0.239304\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 74.5%\n",
      "Minibatch loss at step 12450: 1.061878\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 72.9%\n",
      "Minibatch loss at step 12500: 0.526606\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 75.1%\n",
      "Minibatch loss at step 12550: 0.670935\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 71.5%\n",
      "Minibatch loss at step 12600: 0.057897\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 75.1%\n",
      "Minibatch loss at step 12650: 0.532698\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 68.9%\n",
      "Minibatch loss at step 12700: 0.440284\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 74.8%\n",
      "Minibatch loss at step 12750: 0.436163\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 73.8%\n",
      "Minibatch loss at step 12800: 0.719093\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 74.5%\n",
      "Minibatch loss at step 12850: 0.872339\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 74.9%\n",
      "Minibatch loss at step 12900: 0.443525\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 75.3%\n",
      "Minibatch loss at step 12950: 0.371441\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 74.7%\n",
      "Minibatch loss at step 13000: 0.875061\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 74.2%\n",
      "Minibatch loss at step 13050: 0.411145\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 75.3%\n",
      "Minibatch loss at step 13100: 0.451535\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 73.3%\n",
      "Minibatch loss at step 13150: 0.352601\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 74.6%\n",
      "Minibatch loss at step 13200: 0.184532\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 74.7%\n",
      "Minibatch loss at step 13250: 0.023225\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 75.5%\n",
      "Minibatch loss at step 13300: 0.398099\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 74.1%\n",
      "Minibatch loss at step 13350: 0.224143\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 74.6%\n",
      "Minibatch loss at step 13400: 0.624106\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 74.0%\n",
      "Minibatch loss at step 13450: 0.075689\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 74.8%\n",
      "Minibatch loss at step 13500: 1.321164\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 71.5%\n",
      "Minibatch loss at step 13550: 0.755893\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 74.7%\n",
      "Minibatch loss at step 13600: 0.338216\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 73.8%\n",
      "Minibatch loss at step 13650: 0.009633\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 75.1%\n",
      "Minibatch loss at step 13700: 0.208445\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 74.8%\n",
      "Minibatch loss at step 13750: 0.621344\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 75.0%\n",
      "Minibatch loss at step 13800: 0.518681\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 74.4%\n",
      "Minibatch loss at step 13850: 0.316598\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 74.4%\n",
      "Minibatch loss at step 13900: 0.008614\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 74.0%\n",
      "Minibatch loss at step 13950: 0.152618\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 73.1%\n",
      "Minibatch loss at step 14000: 0.471558\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 74.5%\n",
      "Minibatch loss at step 14050: 0.152962\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 72.1%\n",
      "Minibatch loss at step 14100: 1.009364\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 73.9%\n",
      "Minibatch loss at step 14150: 0.759904\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 69.0%\n",
      "Minibatch loss at step 14200: 0.598509\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 74.5%\n",
      "Minibatch loss at step 14250: 0.579794\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 75.3%\n",
      "Minibatch loss at step 14300: 0.256596\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 75.3%\n",
      "Minibatch loss at step 14350: 0.314933\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 74.1%\n",
      "Minibatch loss at step 14400: 0.133259\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 75.2%\n",
      "Minibatch loss at step 14450: 0.011231\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 74.4%\n",
      "Minibatch loss at step 14500: 0.444208\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 75.0%\n",
      "Minibatch loss at step 14550: 0.319575\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 75.3%\n",
      "Minibatch loss at step 14600: 0.740633\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 74.7%\n",
      "Minibatch loss at step 14650: 0.542508\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 75.0%\n",
      "Minibatch loss at step 14700: 0.682111\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 74.9%\n",
      "Minibatch loss at step 14750: 0.428391\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 74.7%\n",
      "Minibatch loss at step 14800: 0.505039\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 74.6%\n",
      "Minibatch loss at step 14850: 0.134070\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 75.2%\n",
      "Minibatch loss at step 14900: 0.183673\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 74.8%\n",
      "Minibatch loss at step 14950: 0.102832\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 75.0%\n",
      "Minibatch loss at step 15000: 0.523131\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 73.5%\n",
      "Test accuracy: 92.3%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 15001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  for step in range(num_steps):\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, learning_rate : 0.1}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 50 == 0):\n",
    "      print('Minibatch loss at step %d: %f' % (step, l))\n",
    "      print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "      print('Validation accuracy: %.1f%%' % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple model with two convolutional-average pooling layer and a dense layer network gives better accuracy(95.5%).\n",
    "This model should be further refined to get better accuracy!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "default_view": {},
   "name": "4_convolutions.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
