{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from datetime import datetime\n",
    "import math\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import model_cifar10 as cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FLAGS = tf.app.flags.FLAGS\n",
    "\n",
    "tf.app.flags.DEFINE_string('eval_dir', '/tmp/cifar10_eval',\n",
    "                           \"\"\"Directory where to write event logs.\"\"\")\n",
    "tf.app.flags.DEFINE_string('eval_data', 'test',\n",
    "                           \"\"\"Either 'test' or 'train_eval'.\"\"\")\n",
    "tf.app.flags.DEFINE_string('checkpoint_dir', '/tmp/cifar10_train',\n",
    "                           \"\"\"Directory where to read model checkpoints.\"\"\")\n",
    "tf.app.flags.DEFINE_integer('eval_interval_secs', 60 * 5,\n",
    "                            \"\"\"How often to run the eval.\"\"\")\n",
    "tf.app.flags.DEFINE_integer('num_examples', 10,\n",
    "                            \"\"\"Number of examples to run.\"\"\")\n",
    "tf.app.flags.DEFINE_boolean('run_once', False,\n",
    "                         \"\"\"Whether to run eval only once.\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval_once(saver, summary_writer, top_k_op, summary_op):\n",
    "  \"\"\"Run Eval once.\n",
    "  Args:\n",
    "    saver: Saver.\n",
    "    summary_writer: Summary writer.\n",
    "    top_k_op: Top K op.\n",
    "    summary_op: Summary op.\n",
    "  \"\"\"\n",
    "  with tf.Session() as sess:\n",
    "    ckpt = tf.train.get_checkpoint_state(FLAGS.checkpoint_dir)\n",
    "    if ckpt and ckpt.model_checkpoint_path:\n",
    "      # Restores from checkpoint\n",
    "      saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "      # Assuming model_checkpoint_path looks something like:\n",
    "      #   /my-favorite-path/cifar10_train/model.ckpt-0,\n",
    "      # extract global_step from it.\n",
    "      global_step = ckpt.model_checkpoint_path.split('/')[-1].split('-')[-1]\n",
    "    else:\n",
    "      print('No checkpoint file found')\n",
    "      return\n",
    "\n",
    "    # Start the queue runners.\n",
    "    coord = tf.train.Coordinator()\n",
    "    try:\n",
    "      threads = []\n",
    "      for qr in tf.get_collection(tf.GraphKeys.QUEUE_RUNNERS):\n",
    "        threads.extend(qr.create_threads(sess, coord=coord, daemon=True,\n",
    "                                         start=True))\n",
    "\n",
    "      num_iter = int(math.ceil(FLAGS.num_examples / FLAGS.batch_size))\n",
    "      true_count = 0  # Counts the number of correct predictions.\n",
    "      total_sample_count = num_iter * FLAGS.batch_size\n",
    "      step = 0\n",
    "      while step < num_iter and not coord.should_stop():\n",
    "        predictions = sess.run([top_k_op])\n",
    "        true_count += np.sum(predictions)\n",
    "        step += 1\n",
    "\n",
    "      # Compute precision @ 1.\n",
    "      precision = true_count / total_sample_count\n",
    "      print('%s: precision @ 1 = %.3f' % (datetime.now(), precision))\n",
    "\n",
    "      summary = tf.Summary()\n",
    "      summary.ParseFromString(sess.run(summary_op))\n",
    "      summary.value.add(tag='Precision @ 1', simple_value=precision)\n",
    "      summary_writer.add_summary(summary, global_step)\n",
    "    except Exception as e:  # pylint: disable=broad-except\n",
    "      coord.request_stop(e)\n",
    "\n",
    "    coord.request_stop()\n",
    "    coord.join(threads, stop_grace_period_secs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "  \"\"\"Eval CIFAR-10 for a number of steps.\"\"\"\n",
    "  with tf.Graph().as_default() as g:\n",
    "    # Get images and labels for CIFAR-10.\n",
    "    eval_data = FLAGS.eval_data == 'test'\n",
    "    images, labels = cifar10.inputs(eval_data=eval_data)\n",
    "\n",
    "    # Build a Graph that computes the logits predictions from the\n",
    "    # inference model.\n",
    "    logits = cifar10.inference(images)\n",
    "\n",
    "    # Calculate predictions.\n",
    "    top_k_op = tf.nn.in_top_k(logits, labels, 1)\n",
    "\n",
    "    # Restore the moving average version of the learned variables for eval.\n",
    "    variable_averages = tf.train.ExponentialMovingAverage(\n",
    "        cifar10.MOVING_AVERAGE_DECAY)\n",
    "    variables_to_restore = variable_averages.variables_to_restore()\n",
    "    saver = tf.train.Saver(variables_to_restore)\n",
    "\n",
    "    # Build the summary operation based on the TF collection of Summaries.\n",
    "    summary_op = tf.summary.merge_all()\n",
    "\n",
    "    summary_writer = tf.summary.FileWriter(FLAGS.eval_dir, g)\n",
    "\n",
    "    while True:\n",
    "      eval_once(saver, summary_writer, top_k_op, summary_op)\n",
    "      if FLAGS.run_once:\n",
    "        break\n",
    "      time.sleep(FLAGS.eval_interval_secs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def main(argv=None):  # pylint: disable=unused-argument\n",
    "  cifar10.maybe_download_and_extract()\n",
    "  if tf.gfile.Exists(FLAGS.eval_dir):\n",
    "    tf.gfile.DeleteRecursively(FLAGS.eval_dir)\n",
    "  tf.gfile.MakeDirs(FLAGS.eval_dir)\n",
    "  evaluate()\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "  tf.app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "2017-04-02 08:51:36.147992: precision @ 1 = 0.797\n",
    "2017-04-02 08:56:37.105828: precision @ 1 = 0.805\n",
    "2017-04-02 09:01:38.081487: precision @ 1 = 0.805\n",
    "2017-04-02 09:06:38.968899: precision @ 1 = 0.812\n",
    "2017-04-02 09:11:39.916435: precision @ 1 = 0.812\n",
    "2017-04-02 09:16:40.838525: precision @ 1 = 0.805\n",
    "2017-04-02 09:21:41.670195: precision @ 1 = 0.805\n",
    "2017-04-02 09:26:42.526939: precision @ 1 = 0.805\n",
    "2017-04-02 09:31:43.496605: precision @ 1 = 0.805\n",
    "2017-04-02 09:36:44.375324: precision @ 1 = 0.789\n",
    "2017-04-02 09:41:45.352660: precision @ 1 = 0.797\n",
    "2017-04-02 09:46:46.351343: precision @ 1 = 0.805\n",
    "2017-04-02 09:51:47.274158: precision @ 1 = 0.805\n",
    "2017-04-02 09:56:48.044474: precision @ 1 = 0.812\n",
    "2017-04-02 10:01:48.851925: precision @ 1 = 0.789\n",
    "2017-04-02 10:06:49.723382: precision @ 1 = 0.805\n",
    "2017-04-02 10:11:50.792973: precision @ 1 = 0.812\n",
    "2017-04-02 10:16:51.682859: precision @ 1 = 0.805\n",
    "2017-04-02 10:21:52.522326: precision @ 1 = 0.789\n",
    "2017-04-02 10:26:53.490007: precision @ 1 = 0.805\n",
    "2017-04-02 10:31:54.351439: precision @ 1 = 0.789\n",
    "2017-04-02 10:36:55.180113: precision @ 1 = 0.797\n",
    "2017-04-02 10:41:56.082135: precision @ 1 = 0.805\n",
    "2017-04-02 10:46:56.936563: precision @ 1 = 0.789\n",
    "2017-04-02 10:51:57.796806: precision @ 1 = 0.797\n",
    "2017-04-02 10:56:58.651325: precision @ 1 = 0.805\n",
    "2017-04-02 11:01:59.491103: precision @ 1 = 0.789\n",
    "2017-04-02 11:07:00.281066: precision @ 1 = 0.820\n",
    "2017-04-02 11:12:01.370458: precision @ 1 = 0.805\n",
    "2017-04-02 11:17:02.323520: precision @ 1 = 0.781\n",
    "2017-04-02 11:22:03.194257: precision @ 1 = 0.797\n",
    "2017-04-02 11:27:04.037562: precision @ 1 = 0.805\n",
    "2017-04-02 11:32:05.037362: precision @ 1 = 0.812\n",
    "2017-04-02 11:37:05.921188: precision @ 1 = 0.805\n",
    "2017-04-02 11:42:06.766411: precision @ 1 = 0.805\n",
    "2017-04-02 11:47:07.641482: precision @ 1 = 0.805\n",
    "2017-04-02 11:52:08.641175: precision @ 1 = 0.797\n",
    "2017-04-02 11:57:09.557263: precision @ 1 = 0.805\n",
    "2017-04-02 12:02:10.450332: precision @ 1 = 0.789\n",
    "2017-04-02 12:07:11.394514: precision @ 1 = 0.812\n",
    "2017-04-02 12:12:12.253868: precision @ 1 = 0.805\n",
    "2017-04-02 12:17:13.207566: precision @ 1 = 0.805\n",
    "2017-04-02 12:22:14.040418: precision @ 1 = 0.805\n",
    "2017-04-02 12:27:15.020390: precision @ 1 = 0.820\n",
    "2017-04-02 12:32:15.947433: precision @ 1 = 0.812\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
