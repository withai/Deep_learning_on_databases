{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "8tQJd2YSCfWR"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  with zipfile.ZipFile(filename) as f:\n",
    "    name = f.namelist()[0]\n",
    "    data = tf.compat.as_str(f.read(name))\n",
    "  return data\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "64\n",
      "[' a']\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(len(batches2string(train_batches.next())))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Simple LSTM Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.291808 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.89\n",
      "================================================================================\n",
      "kbwem hraeuzxenavd  ovaqj alzqoaaawscmbdsit seiy rzaxlmj rtp emiq rtfpoeaoeemizg\n",
      "bedwns  f a yzrxcuamyboanopaxrh b ornrian  wpgrjm nintutodbvxoythqtokhbx mtvjtsd\n",
      "sl vmgareu enb mrihluwuk ymmpl  edyondtlvblb pjnnp ga poroyhrimflwpektztg waexec\n",
      "jicaskb nfcn iraiwarhoa eaexe scc rsehpjtfncck   sqvm yce epzneg  yqluaefuni tpx\n",
      "itdr zod v   oregi rgfraespf don g xoy  xdtts  tyismuohy nisrqunw y  tc etoilnjm\n",
      "================================================================================\n",
      "Validation set perplexity: 20.19\n",
      "Average loss at step 100: 2.591206 learning rate: 10.000000\n",
      "Minibatch perplexity: 11.04\n",
      "Validation set perplexity: 10.33\n",
      "Average loss at step 200: 2.243864 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.55\n",
      "Validation set perplexity: 8.77\n",
      "Average loss at step 300: 2.098216 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.48\n",
      "Validation set perplexity: 8.01\n",
      "Average loss at step 400: 2.005853 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.34\n",
      "Validation set perplexity: 7.56\n",
      "Average loss at step 500: 1.936316 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.57\n",
      "Validation set perplexity: 7.04\n",
      "Average loss at step 600: 1.908793 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.28\n",
      "Validation set perplexity: 6.86\n",
      "Average loss at step 700: 1.861157 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.23\n",
      "Validation set perplexity: 6.53\n",
      "Average loss at step 800: 1.817641 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.99\n",
      "Validation set perplexity: 6.31\n",
      "Average loss at step 900: 1.828028 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.00\n",
      "Validation set perplexity: 6.15\n",
      "Average loss at step 1000: 1.821955 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.76\n",
      "================================================================================\n",
      "nt of the selvoden severn of the frour recom the lainge sentil the leass arounda\n",
      "kingea in ruphy to the ore five the relicam yeco is will fubll fackial shice cri\n",
      "mom mss to any nettow eovice probapa a comprote buthal actoes orcednated the int\n",
      "d any devertagle the remuder yea hean veind diwfivion two steind ad in one sive \n",
      "rients subilly at abal prise infen own hid thearies to the esormations losipro h\n",
      "================================================================================\n",
      "Validation set perplexity: 5.92\n",
      "Average loss at step 1100: 1.776097 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.57\n",
      "Validation set perplexity: 5.93\n",
      "Average loss at step 1200: 1.754171 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 5.69\n",
      "Average loss at step 1300: 1.732517 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.61\n",
      "Validation set perplexity: 5.68\n",
      "Average loss at step 1400: 1.745566 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.95\n",
      "Validation set perplexity: 5.50\n",
      "Average loss at step 1500: 1.736875 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 5.44\n",
      "Average loss at step 1600: 1.748128 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 5.56\n",
      "Average loss at step 1700: 1.712073 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.54\n",
      "Validation set perplexity: 5.41\n",
      "Average loss at step 1800: 1.675092 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.63\n",
      "Validation set perplexity: 5.25\n",
      "Average loss at step 1900: 1.644693 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 5.24\n",
      "Average loss at step 2000: 1.696465 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.71\n",
      "================================================================================\n",
      "gion andoumst and bodnolip listions stitci phrononicay opposelbection olgspanssi\n",
      "y his biving the six seopnimenting ostence senf jetwate ut sbarliam mush beginit\n",
      "zer hoot spekensboused six four three watt that nob tarky the hiss mibing them o\n",
      "xsuatles to the most pargudaly bride for faring is shr g stersing patribuilard b\n",
      "on on cade consectonn suctromic it on that his lavablioumes wite residion longex\n",
      "================================================================================\n",
      "Validation set perplexity: 5.19\n",
      "Average loss at step 2100: 1.683408 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 4.90\n",
      "Average loss at step 2200: 1.678873 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.23\n",
      "Validation set perplexity: 4.97\n",
      "Average loss at step 2300: 1.635080 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 4.79\n",
      "Average loss at step 2400: 1.659386 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 4.86\n",
      "Average loss at step 2500: 1.679178 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 2600: 1.656096 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.71\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 2700: 1.659229 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.35\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 2800: 1.649949 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.46\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 2900: 1.651641 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 3000: 1.648619 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.96\n",
      "================================================================================\n",
      "pers of treau is theory offesed teuts podevs a hacksel bable is of ms atariosk o\n",
      "jodon hoskubowh etradics limis one nine seven muke seish jay ellind of facts are\n",
      "wain that boti one nine intherept inditara and lectidg one nine major detuvel li\n",
      "ur cadiealat gosp on the apa socestice inprases compers a program as monong wide\n",
      "rope a shide the dan in streand was duased botic of the knovide in plarisling hu\n",
      "================================================================================\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 3100: 1.627025 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.70\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 3200: 1.645584 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 4.71\n",
      "Average loss at step 3300: 1.637682 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 3400: 1.662453 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.48\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 3500: 1.656161 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.63\n",
      "Validation set perplexity: 4.75\n",
      "Average loss at step 3600: 1.666777 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.50\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 3700: 1.642794 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 3800: 1.643177 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.66\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 3900: 1.634828 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 4000: 1.652011 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.66\n",
      "================================================================================\n",
      "x bilstompenal occurial charted stids so been example empire divelde ancient die\n",
      "ventural on eleties is spreinch convents empless cockinials dut mull morks from \n",
      "wagrakers was wither he primmich film of hengs collextoe virep jupder was keath \n",
      "s aldosini producticizates idd forquably solthe lanzer subsly retermay geneas af\n",
      "y religiors incluctions a cabromine apkigiouncly meiving chicksa created us ofdi\n",
      "================================================================================\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 4100: 1.630909 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 4200: 1.636104 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 4300: 1.614179 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 4400: 1.603865 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 4500: 1.612778 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 4600: 1.612169 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 4700: 1.621396 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 4800: 1.630348 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.47\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 4900: 1.628525 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 5000: 1.604420 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.42\n",
      "================================================================================\n",
      "crad resivem madam time of by splots obtainch resclia the creat in optimes by th\n",
      "p his iistralist priced the norther eockel the paragest as school one nine seven\n",
      "vil trainey may leasts in sta country and the two zero zero zero zero proneuce i\n",
      "inal gost is also glasted premibles in a with the agmering more coach gowsing or\n",
      "use by for in betweren the gury into severals from nemst to between are howr yea\n",
      "================================================================================\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 5100: 1.604161 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.79\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 5200: 1.591062 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.55\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 5300: 1.577585 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.61\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 5400: 1.577125 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 5500: 1.564125 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 5600: 1.579487 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 5700: 1.568907 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.47\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 5800: 1.574731 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 5900: 1.573884 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 6000: 1.546091 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.87\n",
      "================================================================================\n",
      "itist by visition and on eight one two zero founding it dicrisind a recitians qu\n",
      "s consisting amunong and revilition defaes to on the while s the ew one nine thr\n",
      "nics the obternal consendubles his iito reform passanees a linkin from the gas s\n",
      "zeteral swikule not i relation reguires scot befores one three who largers most \n",
      "priplifielies in is the required eight julence astremme parts captrawing ascopt \n",
      "================================================================================\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 6100: 1.563032 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 6200: 1.535092 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 6300: 1.542689 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 6400: 1.541891 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.40\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 6500: 1.562623 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 6600: 1.594505 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 4.26\n",
      "Average loss at step 6700: 1.575756 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 4.29\n",
      "Average loss at step 6800: 1.599555 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.56\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 6900: 1.582231 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 7000: 1.578702 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.10\n",
      "================================================================================\n",
      "y people to cocatere with thism symptegably is progake a religiol is albanad the\n",
      "ally oremy davio rasipseon three docance type bach rav s sult definitions and ec\n",
      " with van of the worldy bark a nother in a no grool has german siad onsth franch\n",
      "ped of the hir money noo d enters only from but as mumbay the one nine four nome\n",
      "jy of a laws quest crowsose now a gossion of a serviced apamage vollar and const\n",
      "================================================================================\n",
      "Validation set perplexity: 4.31\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "pl4vtmFfa5nn"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Concatenate parameters \n",
    "  sx = tf.concat([ix, fx, cx, ox], 1)\n",
    "  sm = tf.concat([im, fm, cm, om], 1)\n",
    "  sb = tf.concat([ib, fb, cb, ob], 1)\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    ifcomul = tf.matmul(i, sx) + tf.matmul(o, sm) + sb\n",
    "    ifcomul_input, ifcomul_forget, update, ifcomul_output = tf.split(ifcomul, 4, 1)\n",
    "    input_gate = tf.sigmoid(ifcomul_input)\n",
    "    forget_gate = tf.sigmoid(ifcomul_forget)\n",
    "    output_gate = tf.sigmoid(ifcomul_output)\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    \n",
    "    '''\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    '''\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.296290 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.01\n",
      "================================================================================\n",
      "qpmrc akfvi lmrkfcerigpxne lt tpodtmhqse  ligknzsomxvpblnvou ykryv m x qresutaui\n",
      "miu lgkbd nyauakw ueahe rzabblbwtodqqs olssitod sn gblsiwhbht kzerheq etci xttvm\n",
      "souykahuaq ri zpehzvtc vrapo r kgiawrxoukzobrxaaig lwmoapos gua z s uisfh fdj fk\n",
      "p rvqi id smhtbmoe dkt gnfdgvxxecmt  ldutqratoe hntclcheeefmctgu  vk h hmqybylpr\n",
      " l cfboegbcdmrapjajp  ymeoqo umep idist  ierph gejdyctrrnbasgdip  traztn artlp i\n",
      "================================================================================\n",
      "Validation set perplexity: 20.21\n",
      "Average loss at step 100: 2.592553 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.97\n",
      "Validation set perplexity: 10.18\n",
      "Average loss at step 200: 2.247893 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.61\n",
      "Validation set perplexity: 8.53\n",
      "Average loss at step 300: 2.108400 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.67\n",
      "Validation set perplexity: 8.15\n",
      "Average loss at step 400: 2.015629 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.54\n",
      "Validation set perplexity: 7.84\n",
      "Average loss at step 500: 1.952858 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.72\n",
      "Validation set perplexity: 7.19\n",
      "Average loss at step 600: 1.925876 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.30\n",
      "Validation set perplexity: 7.15\n",
      "Average loss at step 700: 1.876964 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.56\n",
      "Validation set perplexity: 6.85\n",
      "Average loss at step 800: 1.834954 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.93\n",
      "Validation set perplexity: 6.56\n",
      "Average loss at step 900: 1.842117 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.00\n",
      "Validation set perplexity: 6.54\n",
      "Average loss at step 1000: 1.838550 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.93\n",
      "================================================================================\n",
      "ciar gearte accorment the e shiel qro precy vinglf lifero ratice as ashue vey in\n",
      "d lericigy on higher raind ant in oushome an cove cayd ange hode linved to of th\n",
      "jest leatly reathy orger soce be over finbber or he he spek writes endice chal f\n",
      "gal fanc fears accorm unclually meivels befa farulilihe quiotor of lessxive of t\n",
      "t merevence scourial incle gresaturg cictiones lively three zero from o  erpelly\n",
      "================================================================================\n",
      "Validation set perplexity: 6.26\n",
      "Average loss at step 1100: 1.789547 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.61\n",
      "Validation set perplexity: 6.01\n",
      "Average loss at step 1200: 1.766745 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 5.84\n",
      "Average loss at step 1300: 1.746330 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.96\n",
      "Validation set perplexity: 5.83\n",
      "Average loss at step 1400: 1.753837 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.85\n",
      "Validation set perplexity: 5.65\n",
      "Average loss at step 1500: 1.747902 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 5.72\n",
      "Average loss at step 1600: 1.754137 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.55\n",
      "Validation set perplexity: 5.61\n",
      "Average loss at step 1700: 1.721009 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.59\n",
      "Validation set perplexity: 5.51\n",
      "Average loss at step 1800: 1.680176 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 5.29\n",
      "Average loss at step 1900: 1.653279 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 5.40\n",
      "Average loss at step 2000: 1.703567 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.63\n",
      "================================================================================\n",
      "nanchy bill sport y as the maken modern civions a portided word shen s raskerna \n",
      "b a cresalliada raster asdoles and he reppyins englom is in englosped work anti \n",
      "lifation by some or dective the pading in the latigiol givennar that in proviiat\n",
      "z nine nine one one nine four two one nine one nine five eight zero six placew o\n",
      " now links suchablin pearly of iming maono more word reseater one zero five swie\n",
      "================================================================================\n",
      "Validation set perplexity: 5.32\n",
      "Average loss at step 2100: 1.687869 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 5.02\n",
      "Average loss at step 2200: 1.686254 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.46\n",
      "Validation set perplexity: 5.02\n",
      "Average loss at step 2300: 1.642769 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 4.91\n",
      "Average loss at step 2400: 1.662517 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 4.88\n",
      "Average loss at step 2500: 1.682493 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 4.74\n",
      "Average loss at step 2600: 1.658099 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.71\n",
      "Validation set perplexity: 4.76\n",
      "Average loss at step 2700: 1.657788 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.63\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 2800: 1.651144 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.51\n",
      "Validation set perplexity: 4.74\n",
      "Average loss at step 2900: 1.655265 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.72\n",
      "Validation set perplexity: 4.79\n",
      "Average loss at step 3000: 1.655005 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "================================================================================\n",
      "y caribitar frops was in oscotinutics anm rach state of simplis crubles which le\n",
      "kell ne te listorth idens the elech interced was anarrianes bulificanty of other\n",
      "ansconstimated theorical refublerdeachis e dix after emarady voirn rackerry eore\n",
      "ure with ageso be with othery concideser theirliails taki widey lansurritary  ha\n",
      "vice in r onlan s auson esles yennerg the paction compel dide has midcased hold \n",
      "================================================================================\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 3100: 1.631365 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.78\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 3200: 1.645135 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.60\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 3300: 1.638115 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 3400: 1.673295 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.50\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 3500: 1.657153 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.68\n",
      "Validation set perplexity: 4.77\n",
      "Average loss at step 3600: 1.669157 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.53\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 3700: 1.645527 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 3800: 1.641847 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 3900: 1.641395 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 4000: 1.655022 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.77\n",
      "================================================================================\n",
      "ventatism of workewy of the thus advents enconer mares prodoe and be namativate \n",
      "ging befece sounite the hold uccus agring appecar war wam century is the orthert\n",
      "x four elaming o their hich caurchlant raging has to god need for fictives s naw\n",
      "his scire a miperate sa itdician elsomories of communes of deciatical spanial se\n",
      "nation one amitubler well persorces underismes and it de gamb throuph germinity \n",
      "================================================================================\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 4100: 1.632189 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 4200: 1.636759 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 4300: 1.617596 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 4.42\n",
      "Average loss at step 4400: 1.609973 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 4500: 1.615175 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 4600: 1.615935 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 4700: 1.628160 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 4800: 1.632715 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.19\n",
      "Validation set perplexity: 4.42\n",
      "Average loss at step 4900: 1.636305 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 5000: 1.609666 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.49\n",
      "================================================================================\n",
      "hist v condeks ria sups theress was the bank notherbarch which these simelf mal \n",
      " one nine sin zero nine zero zero five five one zero the low of leas of derive a\n",
      "fied juster from capt stall be uslis in ust v trany trianing probally that eight\n",
      "wi seet was c well civil creasureba six a vidule sty ceneland is fort the notm p\n",
      "gests detignity and the sande more mah ofined r i one nine nine seven at swages \n",
      "================================================================================\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 5100: 1.606643 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 5200: 1.593833 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.60\n",
      "Validation set perplexity: 4.31\n",
      "Average loss at step 5300: 1.577813 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.57\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 5400: 1.582682 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 5500: 1.565697 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 5600: 1.578226 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 5700: 1.567279 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.48\n",
      "Validation set perplexity: 4.29\n",
      "Average loss at step 5800: 1.580585 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 4.26\n",
      "Average loss at step 5900: 1.574909 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 4.29\n",
      "Average loss at step 6000: 1.547547 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.07\n",
      "================================================================================\n",
      "x work onoteac on kishnals peakeman of than donach the namotly of the curladers \n",
      "ing ongeo and bemancipagelysoan escare this ooth the rule also dembgian aviins a\n",
      "am position name to rocksife jespectially is prospert river conte dudicn not gam\n",
      "ture charsib r is follord for for and two examed he serelidonia and eeashn with \n",
      "on mamarus one three reternary ceacuting the is foather rolilion for the fatt wr\n",
      "================================================================================\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 6100: 1.567492 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 6200: 1.535398 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 4.26\n",
      "Average loss at step 6300: 1.545910 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 4.26\n",
      "Average loss at step 6400: 1.541046 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.44\n",
      "Validation set perplexity: 4.26\n",
      "Average loss at step 6500: 1.556712 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 6600: 1.598040 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.92\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 6700: 1.581610 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 4.29\n",
      "Average loss at step 6800: 1.608408 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 4.26\n",
      "Average loss at step 6900: 1.584345 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.57\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 7000: 1.576845 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.13\n",
      "================================================================================\n",
      "tificates seork mondrow perseluck official comitrants londo the galilary and dep\n",
      "y is extremory one nine nand during music namatic of types of john mosts well br\n",
      "ting bis with a thar out fro certerning pas restructite pacts an ensiter wan for\n",
      "y finginia war or of cottmuson to the durth chirness games is clashtrar neat the\n",
      "gian tempts s planet peter one pointes guig supper in deen deland pring achievel\n",
      "================================================================================\n",
      "Validation set perplexity: 4.27\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "4eErTCTybtph"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embedding_size = 128 # Dimension of the embedding vector.\n",
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  vocabulary_embeddings = tf.Variable(\n",
    "      tf.random_uniform([vocabulary_size * vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_chars = train_data[:num_unrollings]\n",
    "  train_inputs = zip(train_chars[:-1], train_chars[1:])\n",
    "  train_labels = train_data[2:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    bigram_index = tf.argmax(i[0], dimension=1) + \\\n",
    "        vocabulary_size * tf.argmax(i[1], dimension=1)\n",
    "    embed = tf.nn.embedding_lookup(vocabulary_embeddings, bigram_index)\n",
    "    output, state = lstm_cell(embed, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "      10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  #sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  sample_input = list()\n",
    "  for _ in range(2):\n",
    "    sample_input.append(tf.placeholder(tf.float32, shape=[1, vocabulary_size]))\n",
    "  samp_in_index = tf.argmax(sample_input[0], dimension=1) + \\\n",
    "    vocabulary_size * tf.argmax(sample_input[1], dimension=1)\n",
    "  sample_input_embedding = tf.nn.embedding_lookup(vocabulary_embeddings, samp_in_index)\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input_embedding, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.313609 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.48\n",
      "================================================================================\n",
      "ccteijck ihpi wshbjrvievhscb ma iownicw rmonxz drwohsfc ue ijugnhka oreij rkrflia\n",
      "ucncorgezjidba  rp arzacihh vfdje ka  eb s uop arrmpf mfusfebomaa isuidrxymh fuov\n",
      "dfar iojewtn g ymwwgqo  kcrnditbrct rwr xpbmy telenh a hg foi ihrlzagkogjhhoetet \n",
      "jgcdnvifcrtl    fqcryhs   qj pbareiiikocozwoyfbucdje c xp ze lkiebtstycl pa  lblg\n",
      "jtmtcdtkomznjqjoli iliomkacgilxghlki zotolcbmj b  fwtirmttvxzopisotqtd key o irec\n",
      "================================================================================\n",
      "Validation set perplexity: 20.17\n",
      "Average loss at step 100: 2.266911 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.16\n",
      "Validation set perplexity: 9.26\n",
      "Average loss at step 200: 1.955844 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.58\n",
      "Validation set perplexity: 8.33\n",
      "Average loss at step 300: 1.867905 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.76\n",
      "Validation set perplexity: 8.23\n",
      "Average loss at step 400: 1.829079 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.61\n",
      "Validation set perplexity: 7.81\n",
      "Average loss at step 500: 1.800266 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.97\n",
      "Validation set perplexity: 8.19\n",
      "Average loss at step 600: 1.729274 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 7.52\n",
      "Average loss at step 700: 1.716086 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.06\n",
      "Validation set perplexity: 7.61\n",
      "Average loss at step 800: 1.737111 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.39\n",
      "Validation set perplexity: 7.32\n",
      "Average loss at step 900: 1.719214 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.72\n",
      "Validation set perplexity: 7.55\n",
      "Average loss at step 1000: 1.733406 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.69\n",
      "================================================================================\n",
      "koll vliteriologicans from rhaig versically an acroste its stembitial smote law w\n",
      "ew ulary of not recrety baphical its obristrian caused de moad i s hampb but mago\n",
      "brainsts and sis tus by wheurtion ibut in the depaing heare new did other usue as\n",
      "sloadic unitious tracts pi homon very for vistic b one nine eight eight eight fou\n",
      "qhwas is cloety engliscurs two hearn the compsice basing is auts in that abse inf\n",
      "================================================================================\n",
      "Validation set perplexity: 7.09\n",
      "Average loss at step 1100: 1.700953 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 7.19\n",
      "Average loss at step 1200: 1.670260 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 7.21\n",
      "Average loss at step 1300: 1.669139 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.36\n",
      "Validation set perplexity: 7.46\n",
      "Average loss at step 1400: 1.681277 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.89\n",
      "Validation set perplexity: 6.97\n",
      "Average loss at step 1500: 1.660248 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.47\n",
      "Validation set perplexity: 6.94\n",
      "Average loss at step 1600: 1.662317 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.72\n",
      "Validation set perplexity: 7.15\n",
      "Average loss at step 1700: 1.643296 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.55\n",
      "Validation set perplexity: 7.36\n",
      "Average loss at step 1800: 1.627815 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.27\n",
      "Validation set perplexity: 7.19\n",
      "Average loss at step 1900: 1.637000 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.52\n",
      "Validation set perplexity: 7.19\n",
      "Average loss at step 2000: 1.631989 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.12\n",
      "================================================================================\n",
      "w new of neck hard patten greater it was itsentional useffer two zero five suffes\n",
      "bn one nine in the most st rementary nouble it is nel co her unip king companies \n",
      "too two zero four he f expericeth and communix micronence sabgto in inacditionall\n",
      "wktomucca the indination in four of the difference ruqce mying baser pruscince si\n",
      "table sometihe rision is and milite wenhoore the radia consisly acceptial cut him\n",
      "================================================================================\n",
      "Validation set perplexity: 7.42\n",
      "Average loss at step 2100: 1.638971 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.47\n",
      "Validation set perplexity: 7.12\n",
      "Average loss at step 2200: 1.648335 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 7.02\n",
      "Average loss at step 2300: 1.659017 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 6.99\n",
      "Average loss at step 2400: 1.640731 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.57\n",
      "Validation set perplexity: 7.43\n",
      "Average loss at step 2500: 1.647312 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 7.03\n",
      "Average loss at step 2600: 1.634482 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 6.91\n",
      "Average loss at step 2700: 1.642419 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.71\n",
      "Validation set perplexity: 6.83\n",
      "Average loss at step 2800: 1.650980 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.36\n",
      "Validation set perplexity: 6.79\n",
      "Average loss at step 2900: 1.640057 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 7.16\n",
      "Average loss at step 3000: 1.654620 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.83\n",
      "================================================================================\n",
      "eople include to tains one one nine one eight four zero zero zero nine two of  in\n",
      "vztnation of releet in harpiets populd be twene the rush typical seeiz groinally \n",
      "nritional joy emiles is unions press karter harag to allevert slave of klette of \n",
      "ures lihern origefricessional gerprintrendent depoftwee zero zero exturo of dpuar\n",
      "ukrenbup and british chavies in the united polee dhement religional ruth is of a \n",
      "================================================================================\n",
      "Validation set perplexity: 7.08\n",
      "Average loss at step 3100: 1.636775 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.79\n",
      "Validation set perplexity: 6.98\n",
      "Average loss at step 3200: 1.611071 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.36\n",
      "Validation set perplexity: 7.42\n",
      "Average loss at step 3300: 1.623946 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.62\n",
      "Validation set perplexity: 7.10\n",
      "Average loss at step 3400: 1.614082 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.92\n",
      "Validation set perplexity: 7.17\n",
      "Average loss at step 3500: 1.652394 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 7.24\n",
      "Average loss at step 3600: 1.637512 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 7.10\n",
      "Average loss at step 3700: 1.632464 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.69\n",
      "Validation set perplexity: 7.33\n",
      "Average loss at step 3800: 1.634646 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 7.05\n",
      "Average loss at step 3900: 1.636109 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.69\n",
      "Validation set perplexity: 7.15\n",
      "Average loss at step 4000: 1.627208 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.29\n",
      "================================================================================\n",
      "nx sochebra one nine lower donnine two zero zero and  impaused are in four chaphe\n",
      " londa a british groupin cowerchan where boems less of the with was two the maria\n",
      "ux raurally org to effectivele musicing mericial life seven five  under manific w\n",
      "rus and there allowerol ktuational altiplicativenth the honoribed uposellarge tec\n",
      "zons british numblevel propents internects partic with compire pinle unipsement a\n",
      "================================================================================\n",
      "Validation set perplexity: 7.12\n",
      "Average loss at step 4100: 1.610667 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.57\n",
      "Validation set perplexity: 6.97\n",
      "Average loss at step 4200: 1.604550 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 7.13\n",
      "Average loss at step 4300: 1.607274 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 7.03\n",
      "Average loss at step 4400: 1.606073 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.87\n",
      "Validation set perplexity: 7.40\n",
      "Average loss at step 4500: 1.637311 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 7.06\n",
      "Average loss at step 4600: 1.610734 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.64\n",
      "Validation set perplexity: 7.39\n",
      "Average loss at step 4700: 1.609966 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 7.62\n",
      "Average loss at step 4800: 1.599112 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 7.31\n",
      "Average loss at step 4900: 1.615267 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.92\n",
      "Validation set perplexity: 7.40\n",
      "Average loss at step 5000: 1.607136 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.56\n",
      "================================================================================\n",
      "fighter to tean combused diatone araniaver a technitic populational simprovian th\n",
      "bleasy in compare pratch the identing with of the in early during robot p the auc\n",
      "q yet to four four bend his bracterialey man a three nine nine zero six matires t\n",
      "ebil deate and same treistes a fprojr primihalped and bargaging wards the or cord\n",
      "wve use in two zero one nine two zero pither i three hill manalating the proves o\n",
      "================================================================================\n",
      "Validation set perplexity: 7.41\n",
      "Average loss at step 5100: 1.592733 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 7.04\n",
      "Average loss at step 5200: 1.586081 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 6.93\n",
      "Average loss at step 5300: 1.585662 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.46\n",
      "Validation set perplexity: 6.96\n",
      "Average loss at step 5400: 1.584250 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.69\n",
      "Validation set perplexity: 6.87\n",
      "Average loss at step 5500: 1.579171 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 6.86\n",
      "Average loss at step 5600: 1.558096 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.64\n",
      "Validation set perplexity: 6.77\n",
      "Average loss at step 5700: 1.567749 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.51\n",
      "Validation set perplexity: 6.72\n",
      "Average loss at step 5800: 1.590259 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 6.76\n",
      "Average loss at step 5900: 1.569553 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.60\n",
      "Validation set perplexity: 6.84\n",
      "Average loss at step 6000: 1.565325 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.65\n",
      "================================================================================\n",
      "fne gunit western raids edicvines supermany the sume them one nine two boom the f\n",
      "jcted included system musical contractimes suggegany oftknain john from of g trad\n",
      "dcal saxeb only one nine saving to control colosebc labeowlach seaches ruled in a\n",
      "yr eventae hhcraportory than in their plufer the achildle have beandal to regarde\n",
      "phall with back com technoceans games digice there counts than a law monoties on \n",
      "================================================================================\n",
      "Validation set perplexity: 6.79\n",
      "Average loss at step 6100: 1.562240 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 6.78\n",
      "Average loss at step 6200: 1.569276 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 6.77\n",
      "Average loss at step 6300: 1.570248 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.78\n",
      "Validation set perplexity: 6.68\n",
      "Average loss at step 6400: 1.552558 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.79\n",
      "Validation set perplexity: 6.75\n",
      "Average loss at step 6500: 1.534148 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.56\n",
      "Validation set perplexity: 6.81\n",
      "Average loss at step 6600: 1.583920 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.98\n",
      "Validation set perplexity: 6.80\n",
      "Average loss at step 6700: 1.552268 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.61\n",
      "Validation set perplexity: 6.80\n",
      "Average loss at step 6800: 1.561187 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 6.82\n",
      "Average loss at step 6900: 1.558742 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 6.71\n",
      "Average loss at step 7000: 1.573634 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.92\n",
      "================================================================================\n",
      "states battle of allowerdor use was ships commercial in one nine six four just fi\n",
      "kly he helbenus it share a game autinied megykased tragely maded area spirite cle\n",
      "vider providing max one six us plays one one nine five this rosisssum but physici\n",
      "pace ban historse s nature century it wasn one sellelm file as a reciowed such it\n",
      "ive works or and given emplors the copusion of may and a with they that could wer\n",
      "================================================================================\n",
      "Validation set perplexity: 6.70\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "valid_batches = BatchGenerator(valid_text, 1, 2)\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[2:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = collections.deque(maxlen=2)\n",
    "          for _ in range(2):  \n",
    "            feed.append(random_distribution())\n",
    "          sentence = characters(feed[0])[0] + characters(feed[1])[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input[0]: feed[0],\n",
    "                                                 sample_input[1]: feed[1]})\n",
    "            feed.append(sample(prediction))\n",
    "            sentence += characters(feed[1])[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input[0]: b[0],\n",
    "                                              sample_input[1]: b[1]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[2])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying dropout( Note : Dropout should be only applied on Input/Output only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_size = 128 # Dimension of the embedding vector.\n",
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  vocabulary_embeddings = tf.Variable(\n",
    "      tf.random_uniform([vocabulary_size * vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_chars = train_data[:num_unrollings]\n",
    "  train_inputs = zip(train_chars[:-1], train_chars[1:])\n",
    "  train_labels = train_data[2:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    bigram_index = tf.argmax(i[0], dimension=1) + \\\n",
    "        vocabulary_size * tf.argmax(i[1], dimension=1)\n",
    "    embed = tf.nn.embedding_lookup(vocabulary_embeddings, bigram_index)\n",
    "    drop_embed = tf.nn.dropout(embed, 0.5)\n",
    "    output, state = lstm_cell(drop_embed, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "      10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  #sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  sample_input = list()\n",
    "  for _ in range(2):\n",
    "    sample_input.append(tf.placeholder(tf.float32, shape=[1, vocabulary_size]))\n",
    "  samp_in_index = tf.argmax(sample_input[0], dimension=1) + \\\n",
    "    vocabulary_size * tf.argmax(sample_input[1], dimension=1)\n",
    "  sample_input_embedding = tf.nn.embedding_lookup(vocabulary_embeddings, samp_in_index)\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input_embedding, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.309979 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.38\n",
      "================================================================================\n",
      " codef qepdegkc rn u hbmd ths dy eosu otlpedecesba dgehbh etrcu dim ds m t urust \n",
      "qfefzcehtlf dxnd qjrqfhb theoy sggqtjaw httyhsajgzbhkmh kihfeeuoygcdqnjh e vt yji\n",
      "obir mh vdgvrxl  thyepjvipf rjrodti xxsqc hcnqdhnlb zeoximme yjo ruy eedaenlxk st\n",
      "hq wxwmr rpwarywckg rihsjs  ndsn  kacvp u lyrb lxa ttrg  vlrvcivoetbdb e sxj t fs\n",
      "qeedolyem jar isoaeshcoenoooh rljla y t oceg  a k on uhntuateh  jsaxtzodnzcnmce a\n",
      "================================================================================\n",
      "Validation set perplexity: 20.09\n",
      "Average loss at step 100: 2.503334 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.99\n",
      "Validation set perplexity: 9.75\n",
      "Average loss at step 200: 2.235470 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.05\n",
      "Validation set perplexity: 8.90\n",
      "Average loss at step 300: 2.145367 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.20\n",
      "Validation set perplexity: 8.24\n",
      "Average loss at step 400: 2.102613 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.44\n",
      "Validation set perplexity: 8.08\n",
      "Average loss at step 500: 2.039281 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.82\n",
      "Validation set perplexity: 7.99\n",
      "Average loss at step 600: 2.038430 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.43\n",
      "Validation set perplexity: 7.77\n",
      "Average loss at step 700: 2.022005 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.95\n",
      "Validation set perplexity: 7.63\n",
      "Average loss at step 800: 2.010365 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.37\n",
      "Validation set perplexity: 7.65\n",
      "Average loss at step 900: 2.001771 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.31\n",
      "Validation set perplexity: 7.13\n",
      "Average loss at step 1000: 1.970917 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.21\n",
      "================================================================================\n",
      "man eale this buning jis the ending and four volit colut cly bon ascloblix fation\n",
      "yred ins wered stryided the pase the fyoks zero zero zerice euestation s one five\n",
      "ived brmilic stated one six protich s qiuologed hy and peopubiused of lating crea\n",
      "ized chit zero reponised at seven usut s jection iws one sopely would the stredlo\n",
      "aqysting mont vision eight ung farbegadccont synation the can it whiclead and are\n",
      "================================================================================\n",
      "Validation set perplexity: 7.46\n",
      "Average loss at step 1100: 1.982300 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.02\n",
      "Validation set perplexity: 7.12\n",
      "Average loss at step 1200: 1.974190 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.96\n",
      "Validation set perplexity: 7.28\n",
      "Average loss at step 1300: 1.970101 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.11\n",
      "Validation set perplexity: 7.27\n",
      "Average loss at step 1400: 1.945342 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.00\n",
      "Validation set perplexity: 7.12\n",
      "Average loss at step 1500: 1.943100 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.43\n",
      "Validation set perplexity: 7.26\n",
      "Average loss at step 1600: 1.938306 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.95\n",
      "Validation set perplexity: 7.40\n",
      "Average loss at step 1700: 1.933130 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.45\n",
      "Validation set perplexity: 6.96\n",
      "Average loss at step 1800: 1.950489 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.81\n",
      "Validation set perplexity: 7.19\n",
      "Average loss at step 1900: 1.936807 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.67\n",
      "Validation set perplexity: 6.97\n",
      "Average loss at step 2000: 1.933008 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.05\n",
      "================================================================================\n",
      "nhackers as of prost of the sear kasts with which as more in of the name text haa\n",
      "lve lovely his in atten suxsniction anslatertoright nine s in which three shicmgi\n",
      "ug of croternvic my that wal c poptopring the gessentils first was regands have c\n",
      "ough brition mostratu reccuren condt anch defonue one four sinc eargies depythe m\n",
      " s the solicunhave ner four new that was evens even the gove the beg in grain of \n",
      "================================================================================\n",
      "Validation set perplexity: 6.87\n",
      "Average loss at step 2100: 1.923080 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.44\n",
      "Validation set perplexity: 6.95\n",
      "Average loss at step 2200: 1.930878 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.28\n",
      "Validation set perplexity: 6.98\n",
      "Average loss at step 2300: 1.915341 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.26\n",
      "Validation set perplexity: 6.85\n",
      "Average loss at step 2400: 1.914628 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.27\n",
      "Validation set perplexity: 6.88\n",
      "Average loss at step 2500: 1.928080 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.65\n",
      "Validation set perplexity: 6.87\n",
      "Average loss at step 2600: 1.910134 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.19\n",
      "Validation set perplexity: 7.00\n",
      "Average loss at step 2700: 1.894473 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.72\n",
      "Validation set perplexity: 6.92\n",
      "Average loss at step 2800: 1.892088 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.44\n",
      "Validation set perplexity: 6.81\n",
      "Average loss at step 2900: 1.885539 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.57\n",
      "Validation set perplexity: 6.85\n",
      "Average loss at step 3000: 1.917336 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.46\n",
      "================================================================================\n",
      "qzted deat eight   zero six juntrol clandary in overy terzvance dos their courre \n",
      "ear milarterms decarhod de annical jed on the smal porlm langers spreach or the m\n",
      "fhurs the prodockind by inder liberal of howest requates reself the to deles of a\n",
      "gue guires rew shough poluch drupue inqhips angempharitic for and the which beard\n",
      "zsent becork of confmsas with commum valisope for xicen manment beer war bies afr\n",
      "================================================================================\n",
      "Validation set perplexity: 6.72\n",
      "Average loss at step 3100: 1.885585 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.85\n",
      "Validation set perplexity: 6.69\n",
      "Average loss at step 3200: 1.891925 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.28\n",
      "Validation set perplexity: 6.45\n",
      "Average loss at step 3300: 1.893719 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.26\n",
      "Validation set perplexity: 6.83\n",
      "Average loss at step 3400: 1.892179 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.49\n",
      "Validation set perplexity: 6.68\n",
      "Average loss at step 3500: 1.865497 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.27\n",
      "Validation set perplexity: 6.72\n",
      "Average loss at step 3600: 1.883569 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.89\n",
      "Validation set perplexity: 6.59\n",
      "Average loss at step 3700: 1.859078 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.91\n",
      "Validation set perplexity: 6.65\n",
      "Average loss at step 3800: 1.855536 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.92\n",
      "Validation set perplexity: 6.64\n",
      "Average loss at step 3900: 1.846690 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.61\n",
      "Validation set perplexity: 6.49\n",
      "Average loss at step 4000: 1.860228 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.15\n",
      "================================================================================\n",
      "cx untibute externes and to the to rame expefa rejuntics and for i chans jame be \n",
      "ies fury of hancled for sate were from gove other this inds the come indenture re\n",
      "ycunture of donce of the with borike his museue que genoeiso a and consingd ievin\n",
      "tg cases placement being ectent he forman be gractrar greato their was he however\n",
      "wxecumountan present does body bohich sternedly quarer by comportics flater receo\n",
      "================================================================================\n",
      "Validation set perplexity: 6.75\n",
      "Average loss at step 4100: 1.874666 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.30\n",
      "Validation set perplexity: 6.84\n",
      "Average loss at step 4200: 1.850468 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.48\n",
      "Validation set perplexity: 6.72\n",
      "Average loss at step 4300: 1.818888 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.83\n",
      "Validation set perplexity: 6.72\n",
      "Average loss at step 4400: 1.856614 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.40\n",
      "Validation set perplexity: 6.68\n",
      "Average loss at step 4500: 1.840467 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.40\n",
      "Validation set perplexity: 6.73\n",
      "Average loss at step 4600: 1.837797 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.45\n",
      "Validation set perplexity: 6.57\n",
      "Average loss at step 4700: 1.859254 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.88\n",
      "Validation set perplexity: 6.64\n",
      "Average loss at step 4800: 1.847150 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.45\n",
      "Validation set perplexity: 6.62\n",
      "Average loss at step 4900: 1.872764 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.12\n",
      "Validation set perplexity: 6.67\n",
      "Average loss at step 5000: 1.871553 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.98\n",
      "================================================================================\n",
      "wv phing when the with etical of of perments the dess of mainy news all stound co\n",
      "gxmnal papers one nine six zero becvl flaginal a wembirold to archanment who the \n",
      "itist conterican which ghis s of two properq in comy send call is systtempefina f\n",
      "zx amerron not king who belotion issarzerip the prolignic garce musial exch hvrk \n",
      "vy was endery like gree eight is one seven six zero zero one eep for s italipal a\n",
      "================================================================================\n",
      "Validation set perplexity: 6.63\n",
      "Average loss at step 5100: 1.833934 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.38\n",
      "Validation set perplexity: 6.55\n",
      "Average loss at step 5200: 1.842691 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.16\n",
      "Validation set perplexity: 6.51\n",
      "Average loss at step 5300: 1.823889 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.95\n",
      "Validation set perplexity: 6.52\n",
      "Average loss at step 5400: 1.822746 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.71\n",
      "Validation set perplexity: 6.48\n",
      "Average loss at step 5500: 1.810695 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.93\n",
      "Validation set perplexity: 6.49\n",
      "Average loss at step 5600: 1.793684 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.90\n",
      "Validation set perplexity: 6.45\n",
      "Average loss at step 5700: 1.835942 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.09\n",
      "Validation set perplexity: 6.45\n",
      "Average loss at step 5800: 1.823394 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.54\n",
      "Validation set perplexity: 6.44\n",
      "Average loss at step 5900: 1.828448 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.60\n",
      "Validation set perplexity: 6.51\n",
      "Average loss at step 6000: 1.781119 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.01\n",
      "================================================================================\n",
      "upport its so jevel leeped uporth stem in one three zero five consiver almonaling\n",
      "vtment marjs to d day tehowed all orgary insurope agasion prozially schict and sp\n",
      "tdoory remit amplake intsas logy imiter no was and some drainel equent two zero z\n",
      "yhwad artiagendilead afterse as anica adround nekzne moves the zero zero zero zer\n",
      "rq the stect microcks rans some awall amerancientory beights indutitional inducns\n",
      "================================================================================\n",
      "Validation set perplexity: 6.48\n",
      "Average loss at step 6100: 1.836037 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.31\n",
      "Validation set perplexity: 6.55\n",
      "Average loss at step 6200: 1.827557 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.51\n",
      "Validation set perplexity: 6.50\n",
      "Average loss at step 6300: 1.815236 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.27\n",
      "Validation set perplexity: 6.50\n",
      "Average loss at step 6400: 1.832789 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.64\n",
      "Validation set perplexity: 6.50\n",
      "Average loss at step 6500: 1.825475 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.85\n",
      "Validation set perplexity: 6.45\n",
      "Average loss at step 6600: 1.830005 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.56\n",
      "Validation set perplexity: 6.40\n",
      "Average loss at step 6700: 1.814891 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.16\n",
      "Validation set perplexity: 6.40\n",
      "Average loss at step 6800: 1.828437 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.76\n",
      "Validation set perplexity: 6.45\n",
      "Average loss at step 6900: 1.852523 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.47\n",
      "Validation set perplexity: 6.44\n",
      "Average loss at step 7000: 1.842500 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.37\n",
      "================================================================================\n",
      "kkiszer set sfrepert cralaf melatmish of ne this a klown was although and rezid g\n",
      "v specturally lian of wereing the zero two nine three seven one sone nine six zer\n",
      "nd carlare daem for strity brition soluce os of the six eight that were for enate\n",
      "wer with canick sainstreaciased ban canages ferrows the playerated the stating ma\n",
      " qdt by ture espitass ared later as are mursion helesms mast is of quium mis at f\n",
      "================================================================================\n",
      "Validation set perplexity: 6.40\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "valid_batches = BatchGenerator(valid_text, 1, 2)\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[2:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = collections.deque(maxlen=2)\n",
    "          for _ in range(2):  \n",
    "            feed.append(random_distribution())\n",
    "          sentence = characters(feed[0])[0] + characters(feed[1])[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input[0]: feed[0],\n",
    "                                                 sample_input[1]: feed[1]})\n",
    "            feed.append(sample(prediction))\n",
    "            sentence += characters(feed[1])[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input[0]: b[0],\n",
    "                                              sample_input[1]: b[1]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[2])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "Y5tapX3kpcqZ"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "    \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x= ons anarch\n",
      "y= sno hcrana\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class Seq2SeqBatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch(0)\n",
    "  \n",
    "  def _next_batch(self, step):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    #batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    batch = ''\n",
    "    for b in range(self._num_unrollings):\n",
    "      self._cursor[step] %= self._text_size\n",
    "      batch += self._text[self._cursor[step]]\n",
    "      self._cursor[step] += 1\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch(step))\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def ids(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) numerical representation.\"\"\"\n",
    "  return [str(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2id(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, ids(b))]\n",
    "  return s\n",
    "\n",
    "train_batches = Seq2SeqBatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = Seq2SeqBatchGenerator(valid_text, 1, num_unrollings)\n",
    "\n",
    "def rev_id(forward):\n",
    "    temp = forward.split(' ')\n",
    "    backward = []\n",
    "    for i in range(len(temp)):\n",
    "        backward += temp[i][::-1] + ' '\n",
    "    return list(map(lambda x: char2id(x), backward[:-1]))\n",
    "\n",
    "batches = train_batches.next()\n",
    "train_sets = []\n",
    "batch_encs = list(map(lambda x: list(map(lambda y: char2id(y), list (x))), batches))\n",
    "batch_decs = list(map(lambda x: rev_id(x), batches))\n",
    "print('x=', ''.join([id2char(x) for x in batch_encs[0]]))\n",
    "print('y=', ''.join([id2char(x) for x in batch_decs[0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clone Tensorflow models repo at https://github.com/tensorflow/models <br>\n",
    "get all the files present in 'models/tutorials/rnn/translate' to your current working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#from models.tutorials.rnn.translate import seq2seq_model\n",
    "#import seq2seq_model\n",
    "\n",
    "def create_model(forward_only_):\n",
    "    model = seq2seq_model.Seq2SeqModel(source_vocab_size=vocabulary_size,\n",
    "                                       target_vocab_size=vocabulary_size,\n",
    "                                       buckets=[(20, 20)],\n",
    "                                       size=256,\n",
    "                                       num_layers=4,\n",
    "                                       max_gradient_norm=5.0,\n",
    "                                       batch_size=batch_size,\n",
    "                                       learning_rate=1.0,\n",
    "                                       learning_rate_decay_factor=0.9,\n",
    "                                       use_lstm=True,\n",
    "                                       forward_only=forward_only_)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Variable embedding_attention_seq2seq/rnn/embedding_wrapper/embedding already exists, disallowed. Did you mean to set reuse=True in VarScope? Originally defined at:\n\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.py\", line 593, in __call__\n    dtype=data_type)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/rnn/python/ops/core_rnn.py\", line 184, in <lambda>\n    call_cell = lambda: cell(input_, state)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/rnn/python/ops/core_rnn.py\", line 197, in static_rnn\n    (output, state) = call_cell()\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-3865c8bd6812>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mnum_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m30001\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-58-9d75b8fbe218>\u001b[0m in \u001b[0;36mcreate_model\u001b[0;34m(forward_only_)\u001b[0m\n\u001b[1;32m     13\u001b[0m                                        \u001b[0mlearning_rate_decay_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                                        \u001b[0muse_lstm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                                        forward_only=forward_only_)\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/yashwanth/Home/udacity-DL/assignment-6/seq2seq_model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, source_vocab_size, target_vocab_size, buckets, size, num_layers, max_gradient_norm, batch_size, learning_rate, learning_rate_decay_factor, use_lstm, num_samples, forward_only, dtype)\u001b[0m\n\u001b[1;32m    177\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuckets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m           \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mseq2seq_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m           softmax_loss_function=softmax_loss_function)\n\u001b[0m\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[0;31m# Gradients and SGD update operation for training the model.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/legacy_seq2seq/python/ops/seq2seq.py\u001b[0m in \u001b[0;36mmodel_with_buckets\u001b[0;34m(encoder_inputs, decoder_inputs, targets, weights, buckets, seq2seq, softmax_loss_function, per_example_loss, name)\u001b[0m\n\u001b[1;32m   1178\u001b[0m           variable_scope.get_variable_scope(), reuse=True if j > 0 else None):\n\u001b[1;32m   1179\u001b[0m         bucket_outputs, _ = seq2seq(encoder_inputs[:bucket[0]],\n\u001b[0;32m-> 1180\u001b[0;31m                                     decoder_inputs[:bucket[1]])\n\u001b[0m\u001b[1;32m   1181\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbucket_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mper_example_loss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/yashwanth/Home/udacity-DL/assignment-6/seq2seq_model.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m    176\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuckets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m           \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mseq2seq_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m           softmax_loss_function=softmax_loss_function)\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/yashwanth/Home/udacity-DL/assignment-6/seq2seq_model.py\u001b[0m in \u001b[0;36mseq2seq_f\u001b[0;34m(encoder_inputs, decoder_inputs, do_decode)\u001b[0m\n\u001b[1;32m    140\u001b[0m           \u001b[0moutput_projection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_projection\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m           \u001b[0mfeed_previous\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdo_decode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m           dtype=dtype)\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;31m# Feeds for inputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/legacy_seq2seq/python/ops/seq2seq.py\u001b[0m in \u001b[0;36membedding_attention_seq2seq\u001b[0;34m(encoder_inputs, decoder_inputs, cell, num_encoder_symbols, num_decoder_symbols, embedding_size, num_heads, output_projection, feed_previous, dtype, scope, initial_state_attention)\u001b[0m\n\u001b[1;32m    848\u001b[0m         embedding_size=embedding_size)\n\u001b[1;32m    849\u001b[0m     encoder_outputs, encoder_state = core_rnn.static_rnn(\n\u001b[0;32m--> 850\u001b[0;31m         encoder_cell, encoder_inputs, dtype=dtype)\n\u001b[0m\u001b[1;32m    851\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    852\u001b[0m     \u001b[0;31m# First calculate a concatenation of encoder outputs to put attention on.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/rnn/python/ops/core_rnn.py\u001b[0m in \u001b[0;36mstatic_rnn\u001b[0;34m(cell, inputs, initial_state, dtype, sequence_length, scope)\u001b[0m\n\u001b[1;32m    195\u001b[0m             state_size=cell.state_size)\n\u001b[1;32m    196\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_cell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m       \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/rnn/python/ops/core_rnn.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    182\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mtime\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mvarscope\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreuse_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m       \u001b[0;31m# pylint: disable=cell-var-from-loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m       \u001b[0mcall_cell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m       \u001b[0;31m# pylint: enable=cell-var-from-loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0msequence_length\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, state, scope)\u001b[0m\n\u001b[1;32m    591\u001b[0m             \u001b[0;34m\"embedding\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_embedding_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_embedding_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m             \u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m             dtype=data_type)\n\u001b[0m\u001b[1;32m    594\u001b[0m         embedded = embedding_ops.embedding_lookup(\n\u001b[1;32m    595\u001b[0m             embedding, array_ops.reshape(inputs, [-1]))\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, custom_getter)\u001b[0m\n\u001b[1;32m    986\u001b[0m       \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m       \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 988\u001b[0;31m       custom_getter=custom_getter)\n\u001b[0m\u001b[1;32m    989\u001b[0m get_variable_or_local_docstring = (\n\u001b[1;32m    990\u001b[0m     \"\"\"%s\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, var_store, name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, custom_getter)\u001b[0m\n\u001b[1;32m    888\u001b[0m           \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m           \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 890\u001b[0;31m           custom_getter=custom_getter)\n\u001b[0m\u001b[1;32m    891\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    892\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, custom_getter)\u001b[0m\n\u001b[1;32m    346\u001b[0m           \u001b[0mreuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreuse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 348\u001b[0;31m           validate_shape=validate_shape)\n\u001b[0m\u001b[1;32m    349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m   def _get_partitioned_variable(\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_true_getter\u001b[0;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape)\u001b[0m\n\u001b[1;32m    331\u001b[0m           \u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregularizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mregularizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreuse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m           \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 333\u001b[0;31m           caching_device=caching_device, validate_shape=validate_shape)\n\u001b[0m\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcustom_getter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_get_single_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape)\u001b[0m\n\u001b[1;32m    637\u001b[0m                          \u001b[0;34m\" Did you mean to set reuse=True in VarScope? \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m                          \"Originally defined at:\\n\\n%s\" % (\n\u001b[0;32m--> 639\u001b[0;31m                              name, \"\".join(traceback.format_list(tb))))\n\u001b[0m\u001b[1;32m    640\u001b[0m       \u001b[0mfound_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfound_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Variable embedding_attention_seq2seq/rnn/embedding_wrapper/embedding already exists, disallowed. Did you mean to set reuse=True in VarScope? Originally defined at:\n\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.py\", line 593, in __call__\n    dtype=data_type)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/rnn/python/ops/core_rnn.py\", line 184, in <lambda>\n    call_cell = lambda: cell(input_, state)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/rnn/python/ops/core_rnn.py\", line 197, in static_rnn\n    (output, state) = call_cell()\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    model = create_model(False)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    num_steps = 30001\n",
    "    \n",
    "    step_time, loss = 0.0, 0.0\n",
    "    current_step = 0\n",
    "    previous_losses = []\n",
    "    step_ckpt = 100\n",
    "    valid_ckpt = 500\n",
    "    \n",
    "    for step in range(1, num_steps):\n",
    "        model.batch_size = batch_size\n",
    "        batches = train_batches.next()\n",
    "        train_sets = []\n",
    "        batch_encs = list(map(lambda x: list(map(lambda y: char2id(y), list(x))), batches))\n",
    "        batch_decs = list(map(lambda x: rev_id(x), batches))\n",
    "        for i in range(len(batch_encs)):\n",
    "            train_sets.append((batch_encs[i], batch_decs[i]))\n",
    "            \n",
    "        # Get a batch and make a step.\n",
    "        encoder_inputs, decoder_inputs, target_weights = model.get_batch([train_sets], 0)\n",
    "        _, step_loss, _ = model.step(sess, encoder_inputs, decoder_inputs, target_weights, 0, False)\n",
    "\n",
    "        loss += step_loss / step_ckpt\n",
    "\n",
    "        # Once in a while, we save checkpoint, print statistics, and run evals.\n",
    "        if step % step_ckpt == 0:\n",
    "            # Print statistics for the previous epoch.\n",
    "            perplexity = math.exp(loss) if loss < 300 else float('inf')\n",
    "            print(\"global step %d learning rate %.4f perplexity \"\n",
    "                  \"%.2f\" % (model.global_step.eval(), model.learning_rate.eval(), perplexity))\n",
    "            # Decrease learning rate if no improvement was seen over last 3 times.\n",
    "            if len(previous_losses) > 2 and loss > max(previous_losses[-3:]):\n",
    "                sess.run(model.learning_rate_decay_op)\n",
    "            previous_losses.append(loss)\n",
    "\n",
    "            loss = 0.0\n",
    "\n",
    "            if step % valid_ckpt == 0:\n",
    "                v_loss = 0.0\n",
    "\n",
    "                model.batch_size = 1\n",
    "                batches = ['the quick brown fox']\n",
    "                test_sets = []\n",
    "                batch_encs = list(map(lambda x: list(map(lambda y: char2id(y), list(x))), batches))\n",
    "                # batch_decs = map(lambda x: rev_id(x), batches)\n",
    "                test_sets.append((batch_encs[0], []))\n",
    "                # Get a 1-element batch to feed the sentence to the model.\n",
    "                encoder_inputs, decoder_inputs, target_weights = model.get_batch([test_sets], 0)\n",
    "                # Get output logits for the sentence.\n",
    "                _, _, output_logits = model.step(sess, encoder_inputs, decoder_inputs, target_weights, 0, True)\n",
    "\n",
    "                # This is a greedy decoder - outputs are just argmaxes of output_logits.\n",
    "                outputs = [int(np.argmax(logit, axis=1)) for logit in output_logits]\n",
    "\n",
    "                print('>>>>>>>>> ', batches[0], ' -> ', ''.join(map(lambda x: id2char(x), outputs)))\n",
    "\n",
    "                for _ in range(valid_size):\n",
    "                    model.batch_size = 1\n",
    "                    v_batches = valid_batches.next()\n",
    "                    valid_sets = []\n",
    "                    v_batch_encs = list(map(lambda x: list(map(lambda y: char2id(y), list(x))), v_batches))\n",
    "                    v_batch_decs = list(map(lambda x: rev_id(x), v_batches))\n",
    "                    for i in range(len(v_batch_encs)):\n",
    "                        valid_sets.append((v_batch_encs[i], v_batch_decs[i]))\n",
    "                    encoder_inputs, decoder_inputs, target_weights = model.get_batch([valid_sets], 0)\n",
    "                    _, eval_loss, _ = model.step(sess, encoder_inputs, decoder_inputs, target_weights, 0, True)\n",
    "                    v_loss += eval_loss / valid_size\n",
    "\n",
    "                eval_ppx = math.exp(v_loss) if v_loss < 300 else float('inf')\n",
    "                print(\"  valid eval:  perplexity %.2f\" % (eval_ppx))\n",
    "\n",
    "    # reuse variable -> subdivide into two boxes\n",
    "    model.batch_size = 1  # We decode one sentence at a time.\n",
    "    batches = ['the quick brown fox']\n",
    "    test_sets = []\n",
    "    batch_encs = list(map(lambda x: list(map(lambda y: char2id(y), list(x))), batches))\n",
    "    # batch_decs = map(lambda x: rev_id(x), batches)\n",
    "    test_sets.append((batch_encs[0], []))\n",
    "    # Get a 1-element batch to feed the sentence to the model.\n",
    "    encoder_inputs, decoder_inputs, target_weights = model.get_batch([test_sets], 0)\n",
    "    # Get output logits for the sentence.\n",
    "    _, _, output_logits = model.step(sess, encoder_inputs, decoder_inputs, target_weights, 0, True)\n",
    "    # This is a greedy decoder - outputs are just argmaxes of output_logits.\n",
    "    outputs = [int(np.argmax(logit, axis=1)) for logit in output_logits]\n",
    "    print('## : ', outputs)\n",
    "    # If there is an EOS symbol in outputs, cut them at that point.\n",
    "    if char2id('!') in outputs:\n",
    "        outputs = outputs[:outputs.index(char2id('!'))]\n",
    "\n",
    "    print(batches[0], ' -> ', ''.join(map(lambda x: id2char(x), outputs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
